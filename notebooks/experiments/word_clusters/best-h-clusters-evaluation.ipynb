{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, gzip, os, re, gensim, sys\n",
    "import datetime as dt\n",
    "from collections import Counter\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score, make_scorer, mean_absolute_error as mae, mean_squared_error as mse\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.dummy import DummyRegressor\n",
    "import clustfunc1\n",
    "import importlib\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import chain, combinations\n",
    "from sklearn.feature_selection import RFECV, SelectFpr, f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from itertools import chain, combinations, product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = './heldout_clusteval/'\n",
    "def procclres(fn, foo = foo):\n",
    "    #print(fn)\n",
    "    f = open(foo+fn, 'r')\n",
    "    ls = [[fn] + l.strip().split('\\t') for l in f.readlines()[1:]]\n",
    "    f.close()\n",
    "    #ls.sort(key = lambda x: float(x[4]))\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getress(wb, foo = './heldout_clusteval/'):\n",
    "    ress = []\n",
    "    fns = [fn for fn in os.listdir(foo) if fn.startswith(wb+'_')]\n",
    "    for fn in fns:\n",
    "        ress += procclres(fn, foo)\n",
    "    ress.sort(key = lambda x: float(x[4]))\n",
    "    return ress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Diener_11_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.17096088435374152',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_11_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1729280504719536',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_11_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.17301445578231295',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_11_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17348040415413965',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_11_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1735836123768616',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_13_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17381807248138242',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 5, \"random_state\": 42}'],\n",
       " ['Diener_10_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.1741560860002581',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_14_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17440596857558638',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_10_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17443884735999493',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_12_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17461639116418365',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_13_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17462653892754382',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 5, \"random_state\": 42}'],\n",
       " ['Diener_14_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.1747207851486176',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_12_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17480903906704906',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_12_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17481833758393012',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_12_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.174921719007822',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_12_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17495664073177813',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_14_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17499432828781902',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_14_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17529090308064862',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_11_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17533627798424203',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_12_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17539652968246697',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_12_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17545294580293877',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_11_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1755084488374958',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_10_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17559722992298793',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_12_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17565891104675763',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_11_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17570177284860344',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_10_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17570799690581962',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_12_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17572425032928535',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_10_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1759254535147392',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_13_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17594653491615758',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_13_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1760434868398274',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_12_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17607989163976784',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_13_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17614155525822628',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_13_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17617010184461818',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_12_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17620675787677614',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_13_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17621918074704124',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_13_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1762323450299788',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_12_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1762348008217457',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_12_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17623768568571377',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_13_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17628578018029076',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_13_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17632814054034232',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_12_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.1763516748298803',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_12_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17636207122790998',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_13_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17636571151227273',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_11_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17642408044687546',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_13_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.176436879749343',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_13_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.1764994294438424',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_11_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.1765051600292413',\n",
       "  '{\"alpha\": 0.0001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_11_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.176531153057649',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_13_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17655389127858995',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_11_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17657874644487764',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_11_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17668024508477365',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_13_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.1766835124741691',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_13_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17670832610307585',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_14_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17671038665190933',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_13_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17672394880933676',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_14_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17675159541998428',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_14_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17675500115478146',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_10_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17683817019946954',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_11_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.1768535138960476',\n",
       "  '{\"loss\": \"square\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_13_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17686096120351708',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_12_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17686506339567537',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_14_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17692472484417338',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_14_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17693011966536715',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_12_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.1769824477867182',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_14_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17698818528777252',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_12_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17702896059952528',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_11_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17708996403192556',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_13_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17718995808824264',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_12_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.1772819741100835',\n",
       "  '{\"loss\": \"square\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_11_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17728457610208065',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_12_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17732314151329875',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_11_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17732826687184067',\n",
       "  '{\"alpha\": 0.001, \"normalize\": false}'],\n",
       " ['Diener_12_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17732946437547945',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_14_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17733263362365265',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_14_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1773525087377396',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_12_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1773609885626736',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_12_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17738261233328648',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_12_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1774246846428173',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_12_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17747847620543059',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_13_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17749956634809289',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_14_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1775177051875762',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_14_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17756354298280821',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_13_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.177570446162413',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_14_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.177581433342981',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_12_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1775832602260145',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_14_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17758367619684906',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_10_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17759896129937255',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 5, \"random_state\": 42}'],\n",
       " ['Diener_11_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.1776006247378835',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_12_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17760791469357806',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_14_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17767947342929732',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_13_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17767985633632719',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_13_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1777075859485966',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_13_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17770782460955603',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_12_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1777407892090817',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_14_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17776525752106068',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_12_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17779935095230215',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.5, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_13_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17780891602173568',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_10_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17782210969312218',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_14_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17783744454887968',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_14_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17784562251581676',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_14_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17786457914369416',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_13_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17787063788763463',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_12_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.1778817855791508',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_14_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17790065476069433',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_14_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17796179438616758',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_11_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17797686183224876',\n",
       "  '{\"alpha\": 0.001, \"normalize\": false}'],\n",
       " ['Diener_14_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17798883503881285',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_13_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17799661541849',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_12_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17802091156518632',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_14_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17803109613896304',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['Diener_13_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17803537297737854',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_14_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1780452545430673',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_11_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.1780501956020221',\n",
       "  '{\"alpha\": 0.0001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_11_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17805838220765852',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_14_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17811697342499977',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_14_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1781593603082391',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_12_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17818462151753475',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_10_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.1781869884135518',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_14_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1781910741576899',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_13_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17819441484388837',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_14_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17819796016120845',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_11_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1782018606253287',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_10_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17824870193579437',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_14_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17830224310397008',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_14_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17830485793038245',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_13_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17836342952870476',\n",
       "  '{\"loss\": \"square\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_13_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17838987886860333',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_10_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17843269080220475',\n",
       "  '{\"alpha\": 0.01, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_14_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17844399862118285',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_10_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1784628984861736',\n",
       "  '{\"alpha\": 0.001, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_10_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.17846834613239015',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['Diener_10_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17846838790894265',\n",
       "  '{\"alpha\": 0.0001, \"normalize\": true}'],\n",
       " ['Diener_10_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17847165497196332',\n",
       "  '{\"alpha\": 0.001, \"normalize\": false}'],\n",
       " ['Diener_10_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17847233212134977',\n",
       "  '{\"alpha\": 0.0001, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_10_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17847574079040596',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_10_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17849212796366962',\n",
       "  '{\"alpha\": 1, \"normalize\": true}'],\n",
       " ['Diener_10_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1785122374901269',\n",
       "  '{\"alpha\": 0.001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_10_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17852788064093927',\n",
       "  '{\"alpha\": 0.0001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_11_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.1785341589173643',\n",
       "  '{\"loss\": \"square\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_10_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.17853870605606562',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['Diener_10_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17864152317858645',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_10_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17867186417500996',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_10_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1786798012180489',\n",
       "  '{\"alpha\": 0.001, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_10_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.17872448979591835',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_10_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1787433652417631',\n",
       "  '{\"alpha\": 1, \"normalize\": true}'],\n",
       " ['Diener_11_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17875017865342302',\n",
       "  '{\"alpha\": 0.001, \"normalize\": false}'],\n",
       " ['Diener_10_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17876658333203516',\n",
       "  '{\"alpha\": 0.01, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_13_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17878867462133854',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_14_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.1788182701093001',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_14_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.1788555425922363',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_10_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17887785105997958',\n",
       "  '{\"alpha\": 1, \"normalize\": true}'],\n",
       " ['Diener_10_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.1788904457209355',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_11_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.17892928004535147',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_10_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.178954624325473',\n",
       "  '{\"alpha\": 0.001, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_13_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.17895549886621315',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_13_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17900456520385932',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_13_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17900654066708666',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_10_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.17900867748814892',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['Diener_10_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17903965372495914',\n",
       "  '{\"alpha\": 0.001, \"normalize\": false}'],\n",
       " ['Diener_14_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17904499506983623',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_11_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17905987093085002',\n",
       "  '{\"alpha\": 0.001, \"l1_ratio\": 0.5, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_11_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17907733748337776',\n",
       "  '{\"alpha\": 0.001, \"normalize\": false}'],\n",
       " ['Diener_11_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17908415227190927',\n",
       "  '{\"alpha\": 0.001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_11_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17909469255629423',\n",
       "  '{\"alpha\": 0.1, \"normalize\": false}'],\n",
       " ['Diener_11_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.17911068594104312',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_11_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17911510074357456',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_11_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1791164291884574',\n",
       "  '{\"alpha\": 0.001, \"l1_ratio\": 0.5, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_10_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.1791167546803083',\n",
       "  '{\"alpha\": 0.0001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_10_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17912000052778368',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_10_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.17912652074482588',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['Diener_13_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17913162457154355',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_10_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17913869787456385',\n",
       "  '{\"alpha\": 0.01, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_10_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17914189017772597',\n",
       "  '{\"alpha\": 0.001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_10_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17914795781370557',\n",
       "  '{\"alpha\": 0.1, \"normalize\": false}'],\n",
       " ['Diener_11_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17914915011489885',\n",
       "  '{\"alpha\": 0.1, \"normalize\": false}'],\n",
       " ['Diener_12_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.17915274083703892',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_11_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17915327084794125',\n",
       "  '{\"alpha\": 0.001, \"l1_ratio\": 0.5, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_10_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.1791569716102042',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_11_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17920718727585086',\n",
       "  '{\"alpha\": 0.001, \"normalize\": false}'],\n",
       " ['Diener_14_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.1792225837871591',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_11_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1792366729670285',\n",
       "  '{\"alpha\": 100, \"normalize\": true}'],\n",
       " ['Diener_12_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17923843665648037',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_11_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.1792413731555943',\n",
       "  '{\"alpha\": 0.01, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_11_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1792497612728984',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_14_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17925303444840054',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_10_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.17928382317063635',\n",
       "  '{\"alpha\": 0.001, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_11_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17928519601287646',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_11_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17928650337966576',\n",
       "  '{\"alpha\": 0.01, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_10_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.17930696919716965',\n",
       "  '{\"alpha\": 1, \"normalize\": false}'],\n",
       " ['Diener_11_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17931216794063823',\n",
       "  '{\"alpha\": 0.01, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_11_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17932607455178953',\n",
       "  '{\"alpha\": 0.01, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_10_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.17934926639661375',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['Diener_10_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17935408952906687',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_13_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17940986484967064',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_12_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17942852630335387',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_10_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1794431666314126',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_14_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.1794594103479264',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_10_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17949824929676197',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_11_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17955475100871526',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['Diener_14_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17957608524046695',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_10_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.17959651811074512',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['Diener_13_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17967706519248877',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_13_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17968549998050712',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_13_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17968549998050712',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_12_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17968549998050712',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_12_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17968549998050712',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_12_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.17968549998050712',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['Diener_10_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.17977962749535736',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['Diener_10_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1797881235827664',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_10_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.17994387755895744',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['Diener_10_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18003259637188207',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_11_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.18019469011704511',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_10_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18029478458049888',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_10_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.1804948300137638',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_12_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18058106575963717',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_13_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.18065507186256713',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['Diener_11_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18078656462585035',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_10_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.18102077430672323',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['Diener_12_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.18146609249417595',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_10_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18161989795918368',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_12_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.18210149384631652',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['Diener_11_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1826268424036281',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_13_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18312145691609977',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_10_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18330498866213152',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_14_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1834041950113379',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_12_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18508219954648525',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_11_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18508928571428573',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_14_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.185328089569161',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_12_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18563633786848072',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_12_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1857482993197279',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_14_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18611961451247166',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_12_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1861238662131519',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_12_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18635841836734696',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_11_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1867049319727891',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_10_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18675170068027208',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_14_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18714143990929705',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_13_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18732426303854877',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_13_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18742772108843533',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_13_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.188625283446712',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_12_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1890965136054422',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_13_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18917233560090702',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_12_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18930272108843538',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_14_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.18998795351473924',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_14_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.19002551020408162',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_13_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1913796768707483',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_14_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.19167800453514738',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_14_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.19173185941043083',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_13_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.19226261337868483',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['Diener_11_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.25632017083352376',\n",
       "  '{\"normalize\": true, \"random_state\": 42}'],\n",
       " ['Diener_11_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.2574221172344266',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['Diener_11_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.2714413200629466',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['Diener_11_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.28712024727800867',\n",
       "  '{\"normalize\": true, \"random_state\": 42}'],\n",
       " ['Diener_11_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.4175493449927813',\n",
       "  '{\"normalize\": true, \"random_state\": 42}'],\n",
       " ['Diener_11_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.4204685199914996',\n",
       "  '{\"normalize\": true, \"random_state\": 42}'],\n",
       " ['Diener_11_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.46876714656227375',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['Diener_11_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.4888104353148567',\n",
       "  '{\"normalize\": true, \"random_state\": 42}'],\n",
       " ['Diener_12_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.7044193977366289',\n",
       "  '{\"normalize\": true, \"random_state\": 42}'],\n",
       " ['Diener_12_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.745405064696609',\n",
       "  '{\"normalize\": true, \"random_state\": 42}'],\n",
       " ['Diener_12_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.9119409754615907',\n",
       "  '{\"normalize\": true, \"random_state\": 42}'],\n",
       " ['Diener_12_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.9314315657641792',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['Diener_12_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1.0145082668652914',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['Diener_12_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1.2253623574310182',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['Diener_13_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['Diener_13_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['Diener_13_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['Diener_14_100.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['Diener_13_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['Diener_13_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['Diener_14_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['Diener_13_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['Diener_14_50.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['Diener_13_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['Diener_14_500.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['Diener_14_100.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['Diener_13_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['Diener_12_1.0_0.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['Diener_12_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['Diener_14_500.0_1.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['Diener_14_50.0_1.0VERYRARE.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['Diener_14_1.0_0.0.txt',\n",
       "  'diener',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}']]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getress('Diener', foo = './heldout_clusteval_noRFE/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['WHO_11_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1541161515681865',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_11_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15424440839005044',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_14_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15455674519690255',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_12_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1546650971585279',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_12_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15487563524345588',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_13_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15515481367017236',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 5, \"random_state\": 42}'],\n",
       " ['WHO_12_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15515486846019472',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 5, \"random_state\": 42}'],\n",
       " ['WHO_10_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15519144834741497',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_14_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15520891459198877',\n",
       "  '{\"alpha\": 10, \"normalize\": true}'],\n",
       " ['WHO_13_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15521281593625305',\n",
       "  '{\"alpha\": 10, \"normalize\": true}'],\n",
       " ['WHO_14_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15521471848755936',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_13_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1552511993859585',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_12_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15536961511011996',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_12_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15544842417203947',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_14_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1555703070698422',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_13_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15560783465714118',\n",
       "  '{\"alpha\": 10, \"normalize\": true}'],\n",
       " ['WHO_13_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1556343716455209',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_11_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15564017727749938',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_12_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15568242746700106',\n",
       "  '{\"alpha\": 10, \"normalize\": true}'],\n",
       " ['WHO_14_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15569885609609269',\n",
       "  '{\"alpha\": 10, \"normalize\": true}'],\n",
       " ['WHO_12_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15571843269111763',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_12_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15574850341800234',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['WHO_12_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15594535127350456',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_12_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15595452014981365',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_13_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1560712054331675',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['WHO_12_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1561346536465018',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['WHO_14_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15615778352255932',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_14_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15620950347382204',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['WHO_12_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15621367105705924',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_12_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1562199788918765',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_12_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15621998231725603',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['WHO_10_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15624576184761851',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_13_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15628602474677913',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_14_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1563142446070105',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_13_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1563413498441627',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['WHO_13_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15636792075924183',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_13_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1563695953907852',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_14_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15643100169836105',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['WHO_13_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15643260670525044',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_13_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15644049031726065',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_13_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1564511357301782',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['WHO_14_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15645716729539458',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_14_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15646027838946916',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['WHO_14_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15651148904154616',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_14_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1565303792945604',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_12_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15654264817515653',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_11_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15654572598646194',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_13_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15663040809468287',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['WHO_11_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15663932893773486',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_12_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15663937743436923',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_13_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15666038860222745',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_13_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1567014550924551',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_14_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15676080445549928',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_14_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15676539497946068',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_14_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15683297004679655',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['WHO_11_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1568544457460992',\n",
       "  '{\"alpha\": 10, \"normalize\": true}'],\n",
       " ['WHO_10_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.15685884353741497',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_12_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15692993526038018',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_13_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15694302323884626',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_13_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1570163857846931',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['WHO_14_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15703842117086247',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['WHO_14_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15704424521160648',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['WHO_13_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15709344558388322',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_12_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15713925578772908',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_11_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15714382981166683',\n",
       "  '{\"alpha\": 10, \"normalize\": true}'],\n",
       " ['WHO_12_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15718042262767132',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_12_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15719662606739093',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_13_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15720270083462806',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['WHO_14_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1572066041225467',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_12_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15722691976168007',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['WHO_13_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15722762293320222',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_13_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15723363140650537',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['WHO_12_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1572386747398325',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['WHO_14_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15725675957814828',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['WHO_13_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.15726360544217685',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_14_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1573345075165514',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_14_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15735696281720962',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_10_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15738862303813636',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_11_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1574005104739667',\n",
       "  '{\"alpha\": 10, \"normalize\": true}'],\n",
       " ['WHO_12_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15742253520250132',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_10_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15744030133140402',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_14_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1574686749213548',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_11_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15746999429117756',\n",
       "  '{\"alpha\": 10, \"normalize\": true}'],\n",
       " ['WHO_12_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1574828193683726',\n",
       "  '{\"alpha\": 0.01, \"normalize\": false}'],\n",
       " ['WHO_12_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15754736796048863',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_14_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15759937859538806',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_11_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15767472844315594',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_13_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1576905258032332',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_12_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15769825027054868',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_12_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15770143288644162',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_12_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15771154083425082',\n",
       "  '{\"alpha\": 100, \"normalize\": true}'],\n",
       " ['WHO_14_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15771495702699706',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_14_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1578061224489796',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_11_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15784578903838936',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_10_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1578625864964633',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_13_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15787697873575102',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_12_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15788296057004908',\n",
       "  '{\"alpha\": 0.0001, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_13_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15796599218385204',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_11_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15796826670835099',\n",
       "  '{\"alpha\": 0.01, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_10_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1580076530612245',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_14_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15804188849067544',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['WHO_11_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.15807142857142858',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_13_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15807599621232055',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_12_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15812332558650422',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_13_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15812565856613742',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_11_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1581260611088468',\n",
       "  '{\"alpha\": 0.01, \"l1_ratio\": 0.5, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_12_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15823225858753448',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_13_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15823962126084318',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_12_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15824089267132563',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_13_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15827707923088757',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_14_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15828359814964316',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['WHO_11_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15831086769653335',\n",
       "  '{\"alpha\": 10, \"normalize\": true}'],\n",
       " ['WHO_11_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15831400664135756',\n",
       "  '{\"alpha\": 0.01, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_11_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15840460164944373',\n",
       "  '{\"alpha\": 10, \"normalize\": true}'],\n",
       " ['WHO_11_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1584177494088616',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_13_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15850091923614457',\n",
       "  '{\"max_depth\": 3, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_10_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15851487053135926',\n",
       "  '{\"alpha\": 1, \"normalize\": true}'],\n",
       " ['WHO_11_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15854142909537783',\n",
       "  '{\"alpha\": 10, \"normalize\": true}'],\n",
       " ['WHO_11_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15855074102863997',\n",
       "  '{\"alpha\": 0.01, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_10_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15858828317079282',\n",
       "  '{\"alpha\": 0.01, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_14_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15863764861927324',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_10_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15867692651547477',\n",
       "  '{\"alpha\": 0.01, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_13_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15868130061832564',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_11_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15868513192355888',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_11_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1586949186833236',\n",
       "  '{\"alpha\": 0.1, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_11_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15869962003983934',\n",
       "  '{\"alpha\": 100, \"normalize\": true}'],\n",
       " ['WHO_13_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15870387250701495',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_14_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15872005369492434',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_13_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15873072654500853',\n",
       "  '{\"alpha\": 0.001, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_14_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15877586404371052',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['WHO_10_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15880117207918473',\n",
       "  '{\"alpha\": 0.01, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_12_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1588044217687075',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_14_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1588292648204889',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_10_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1588407096939403',\n",
       "  '{\"alpha\": 0.01, \"l1_ratio\": 0.75, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_11_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.1588496158714546',\n",
       "  '{\"alpha\": 0.1, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_10_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1588609211533652',\n",
       "  '{\"alpha\": 10, \"normalize\": true}'],\n",
       " ['WHO_10_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1588630007578072',\n",
       "  '{\"alpha\": 10, \"normalize\": true}'],\n",
       " ['WHO_11_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15886530630924117',\n",
       "  '{\"alpha\": 0.1, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_10_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15886960870024802',\n",
       "  '{\"alpha\": 10, \"normalize\": true}'],\n",
       " ['WHO_11_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15887686777893328',\n",
       "  '{\"alpha\": 0.1, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": true, \"random_state\": 42, \"selection\": \"random\"}'],\n",
       " ['WHO_10_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_10_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_13_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_10_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_10_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_12_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_14_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_10_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_10_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_13_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_14_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_10_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_10_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_11_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_11_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_10_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_10_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_14_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_14_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_14_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_10_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_10_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'ElasticNet',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"l1_ratio\": 0.25, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_11_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_11_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_12_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_11_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_11_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_11_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_11_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_14_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Lasso',\n",
       "  '0.15887766569277786',\n",
       "  '{\"alpha\": 100, \"max_iter\": 500, \"normalize\": false, \"random_state\": 42, \"selection\": \"cyclic\"}'],\n",
       " ['WHO_10_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1588792669677638',\n",
       "  '{\"alpha\": 100, \"normalize\": false}'],\n",
       " ['WHO_10_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.15887992255563685',\n",
       "  '{\"alpha\": 100, \"normalize\": false}'],\n",
       " ['WHO_10_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1588806445894745',\n",
       "  '{\"alpha\": 100, \"normalize\": false}'],\n",
       " ['WHO_10_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'Ridge',\n",
       "  '0.1588808307882058',\n",
       "  '{\"alpha\": 100, \"normalize\": false}'],\n",
       " ['WHO_12_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15902198675689805',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_12_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15904443158473316',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['WHO_10_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.15911543317660673',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['WHO_11_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15915345823297827',\n",
       "  '{\"loss\": \"square\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['WHO_10_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15915569634924503',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['WHO_12_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.15920493197278912',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_13_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.1593342958115204',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_12_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.1593367776071316',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_10_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.15938876544912114',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['WHO_12_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.1593958273653437',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['WHO_14_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.15941666666666665',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_14_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.15942006802721087',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_11_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1594251700680272',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_13_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15942654114120353',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_10_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1594426591428765',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_10_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15953306598299258',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_13_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.15955272108843538',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_10_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.15956836959770643',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['WHO_14_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15957622864546694',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['WHO_12_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1596173469387755',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_10_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.15969728012097292',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['WHO_10_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1596998299319728',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_10_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.15970153061224487',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_10_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.15971391186087897',\n",
       "  '{\"normalize\": true, \"random_state\": 42}'],\n",
       " ['WHO_10_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.15976907611662214',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['WHO_10_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.1597725301978829',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['WHO_14_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15981880082567784',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['WHO_11_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.15982397959183672',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_10_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15983294414957486',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['WHO_11_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15984187138019862',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_10_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15986348876333628',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_10_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.15987363850433484',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_10_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.15987750393987604',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_10_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.15999595952622048',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['WHO_11_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.16006231758229594',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_11_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1601113945578231',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_10_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'RandomForestRegressor',\n",
       "  '0.1602594972840462',\n",
       "  '{\"max_depth\": 2, \"max_features\": \"auto\", \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"n_estimators\": 20, \"random_state\": 42}'],\n",
       " ['WHO_13_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.16030002065757365',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_13_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.16034644343771004',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_11_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.1604337038554578',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_11_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.16046067181305232',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_14_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1609098639455782',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_12_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.16097432263061923',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['WHO_10_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.16104878129669525',\n",
       "  '{\"loss\": \"square\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_10_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.16108119488532174',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_13_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.16115610407453831',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['WHO_12_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.16124765143933564',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_11_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.161343537414966',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_12_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1613996598639456',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_14_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.16171214625962627',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_10_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.16179081632653064',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_11_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1618469387755102',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_11_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.1620715878098859',\n",
       "  '{\"loss\": \"exponential\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_12_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1623860544217687',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_10_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1625102040816327',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_12_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.16266156462585032',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_10_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1626658163265306',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_10_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.16284503727460814',\n",
       "  '{\"loss\": \"linear\", \"n_estimators\": 100, \"random_state\": 42}'],\n",
       " ['WHO_10_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.163062925170068',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_14_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1634812925170068',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_11_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'AdaBoostRegressor',\n",
       "  '0.16362057506567865',\n",
       "  '{\"loss\": \"square\", \"n_estimators\": 10, \"random_state\": 42}'],\n",
       " ['WHO_14_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.16363775510204082',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_14_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.16367176870748298',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_14_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.164296768707483',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_13_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.16436309523809525',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_13_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.16441836734693877',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_11_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.16487074829931972',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_12_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.16514115646258504',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_13_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1654234693877551',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_13_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1662857142857143',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_13_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.16660544217687073',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_11_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1666122448979592',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_13_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1668554421768708',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_12_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'DecisionTreeRegressor',\n",
       "  '0.1669625850340136',\n",
       "  '{\"criterion\": \"mae\", \"max_depth\": 2, \"max_features\": \"auto\", \"max_leaf_nodes\": 3, \"min_samples_leaf\": 2, \"min_samples_split\": 2, \"random_state\": 42, \"splitter\": \"best\"}'],\n",
       " ['WHO_11_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.20994649819485378',\n",
       "  '{\"normalize\": true, \"random_state\": 42}'],\n",
       " ['WHO_11_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.23375722253489398',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['WHO_11_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.24152622963847756',\n",
       "  '{\"normalize\": true, \"random_state\": 42}'],\n",
       " ['WHO_11_1.0_0.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.24511120923234161',\n",
       "  '{\"normalize\": true, \"random_state\": 42}'],\n",
       " ['WHO_11_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.3225184058754983',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['WHO_11_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.3403209274215004',\n",
       "  '{\"normalize\": true, \"random_state\": 42}'],\n",
       " ['WHO_11_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.3405539715574624',\n",
       "  '{\"normalize\": true, \"random_state\": 42}'],\n",
       " ['WHO_11_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.34390794082800835',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['WHO_12_500.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.5419299722866256',\n",
       "  '{\"normalize\": true, \"random_state\": 42}'],\n",
       " ['WHO_12_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.5575753473189181',\n",
       "  '{\"normalize\": true, \"random_state\": 42}'],\n",
       " ['WHO_12_100.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.7924273528632401',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['WHO_12_50.0_1.0.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.8255965268802289',\n",
       "  '{\"normalize\": true, \"random_state\": 42}'],\n",
       " ['WHO_12_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.8433059950017633',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['WHO_12_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '0.8907644449836505',\n",
       "  '{\"normalize\": false, \"random_state\": 42}'],\n",
       " ['WHO_14_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['WHO_13_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['WHO_13_50.0_1.0.txt', 'who', 'RFECV', 'LinearRegression', '1000', '{}'],\n",
       " ['WHO_12_1.0_0.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['WHO_14_100.0_1.0.txt', 'who', 'RFECV', 'LinearRegression', '1000', '{}'],\n",
       " ['WHO_13_1.0_0.0.txt', 'who', 'RFECV', 'LinearRegression', '1000', '{}'],\n",
       " ['WHO_14_50.0_1.0.txt', 'who', 'RFECV', 'LinearRegression', '1000', '{}'],\n",
       " ['WHO_14_1.0_0.0.txt', 'who', 'RFECV', 'LinearRegression', '1000', '{}'],\n",
       " ['WHO_13_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['WHO_14_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['WHO_14_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['WHO_13_500.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['WHO_14_500.0_1.0.txt', 'who', 'RFECV', 'LinearRegression', '1000', '{}'],\n",
       " ['WHO_12_1.0_0.0.txt', 'who', 'RFECV', 'LinearRegression', '1000', '{}'],\n",
       " ['WHO_13_100.0_1.0.txt', 'who', 'RFECV', 'LinearRegression', '1000', '{}'],\n",
       " ['WHO_13_500.0_1.0.txt', 'who', 'RFECV', 'LinearRegression', '1000', '{}'],\n",
       " ['WHO_14_100.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}'],\n",
       " ['WHO_13_50.0_1.0VERYRARE.txt',\n",
       "  'who',\n",
       "  'RFECV',\n",
       "  'LinearRegression',\n",
       "  '1000',\n",
       "  '{}']]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getress('WHO', foo = './heldout_clusteval_noRFE/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfoo = '../gold_split/'\n",
    "wdf2 = pd.read_csv(gfoo + 'heldout-only-words1year.csv', index_col=0)\n",
    "\n",
    "freqsdf = pd.read_csv(gfoo + 'heldout-freqs.csv', index_col = 0)\n",
    "def clusterdf(fn, rare = False):\n",
    "    cdf = pd.read_csv(fn, index_col = 0)\n",
    "    resd, centerd = {}, {}\n",
    "    ids, centers = [], []\n",
    "    for i, r in cdf.iterrows():\n",
    "        #if rare or (r.WHO_P < 0.05) or (r.Diener_P < 0.05):\n",
    "            ws = r.ex.strip().split(', ')\n",
    "            for w in ws:\n",
    "                resd[w] = i\n",
    "            if rare:\n",
    "                ids.append(i)\n",
    "                centers.append(np.mean([model[w] for w in ws], axis=0))\n",
    "                \n",
    "            #print(i)\n",
    "    if rare:\n",
    "        cls = []\n",
    "        for w in wdf2.columns:\n",
    "            cl = resd.get(w, -1)\n",
    "            \n",
    "            if (cl == -1) and (w in model.vocab) and (w in freqsdf.index) and (freqsdf.loc[w, '0'] >= 150):\n",
    "                \n",
    "                dists = [cosine(model[w], center) for center in centers]\n",
    "                dists = [d if d >= 0. else 1 for d in dists ]\n",
    "                #cl1 = ids[np.argmin(dists)]\n",
    "            if (cl != -1) and ((cdf.loc[cl, 'WHO_P'] < 0.05) or (cdf.loc[cl, 'Diener_P'] < 0.05)):\n",
    "                #cl = cl\n",
    "                print(w, cl, cdf.loc[cl, 'WHO_P'], cdf.loc[cl, 'Diener_P'])\n",
    "                cls.append(cl)\n",
    "            else:\n",
    "                cls.append(-1)\n",
    "        print(cls)\n",
    "    else:\n",
    "        cls = np.asarray([resd.get(w, -1) for w in wdf2.columns])\n",
    "    cldf = wdf2.groupby(by = cls, axis = 'columns').sum()\n",
    "    #print(cldf.shape)\n",
    "    cldf.drop(-1, axis=1, inplace = True)\n",
    "    return cldf\n",
    "\n",
    "    \n",
    "rat = {'who': 25., 'diener': 30.}\n",
    "metrs = ['MAE', 'Pearson', 'R-2']\n",
    "def set_params(func, parameters):\n",
    "    for parameter, value in parameters.items():\n",
    "        if hasattr(func, parameter):\n",
    "            setattr(func, parameter, value)\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_nestedCV(X, y, wb, step=1, verbose=0, method='', fsname='', funcname='Ridge', fout=None, fout_mean=None, cvn=10):\n",
    "    grids = parameters[funcname]\n",
    "    print(funcname, flush=True)\n",
    "    wb = wb.lower()\n",
    "    sortedarg = np.asarray(y.argsort())\n",
    "    cv = []\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    for ti in range(cvn):\n",
    "        \n",
    "            test = sortedarg[ti::cvn]\n",
    "            train = np.setdiff1d(sortedarg, test)\n",
    "            cv.append([train, test])\n",
    "    \n",
    "    maes, prs, r2s = [], [], []\n",
    "    \n",
    "    best_params = {}\n",
    "        \n",
    "    bestmae = 1000\n",
    "    #\n",
    "    #print(len(train), len(dev), len(test))\n",
    "    \n",
    "    for grid in grids:\n",
    "        keys = sorted(grid.keys())\n",
    "        for comb in product(*[val for k, val in sorted(grid.items())]):\n",
    "            print(comb, flush=True)\n",
    "            devmaes = []\n",
    "            #for i, [train, test] in enumerate(cv):\n",
    "            #    print(i, 'CV fold..', flush=True)\n",
    "            model = set_params(deepcopy(models[funcname]), dict(zip(keys, comb)))\n",
    "                #lr = model\n",
    "            #    if verbose:\n",
    "            #        print('Model:', funcname, 'params:', dict(zip(keys, comb)), 'est:', model)\n",
    "\n",
    "            fs = RFECV(model, cv=cv, scoring=make_scorer(mae, greater_is_better=False), step=step, verbose=verbose)\n",
    "                #print(X.shape, y.shape)\n",
    "            fs.fit(X, y)\n",
    "            if verbose:\n",
    "                print(fs.n_features_, flush=True)\n",
    "            resmae = fs.grid_scores_.max()/-rat[wb]\n",
    "            if verbose:\n",
    "                    print(resmae, flush=True)\n",
    "                    #print('Others:',fs.grid_scores_.mean()/-rat[wb], fs.grid_scores_.min()/-rat[wb])\n",
    "            devmaes.append(resmae)\n",
    "            devmae = np.mean(devmaes)\n",
    "            if devmae < bestmae:\n",
    "                    bestmae = devmae\n",
    "                    best_params = dict(zip(keys, comb))\n",
    "                    best_params['fs'] = fs\n",
    "                    best_params['fs_sup'] = int(sum(fs.get_support()==1))\n",
    "                    \n",
    "    print('Dev: ', bestmae, best_params, flush=True)\n",
    "    '''est = set_params(deepcopy(models[funcname]), best_params)                                 \n",
    "    est.fit(best_params['fs'].transform(X[train]), y[train])\n",
    "    testpred = est.predict(best_params['fs'].transform(X[test]))\n",
    "\n",
    "    cur_mae = mae(y[test], testpred)/rat[wb]\n",
    "    maes.append(cur_mae)\n",
    "    cur_prs = pearsonr(y[test], testpred)[0]\n",
    "    prs.append(cur_prs)\n",
    "    cur_r2 = r2_score(y[test], testpred)\n",
    "    r2s.append(cur_r2)\n",
    "    '''\n",
    "    if fout is not None:\n",
    "                fout.write('{0}\\t{1}\\t{2}\\t{3}\\t{4}\\n'.format(wb, method, funcname, \n",
    "                                                                              bestmae, \n",
    "                                                                              json.dumps({k: v for k, v in best_params.items() if k != 'fs'})))\n",
    "                fout.flush()\n",
    "    #print('Test: ', [round(np.mean(x), 4) for x in [maes, prs, r2s]], flush=True)\n",
    "    if fout_mean is not None:\n",
    "        fout_mean.write('{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\\n'.format(method, wb, fsname, funcname,\n",
    "                                                                     round(np.mean(devmaes), 4),\n",
    "                                                                     round(np.mean(maes), 4),\n",
    "                                                                     round(np.mean(prs), 4),\n",
    "                                                                     round(np.mean(r2s), 4)))\n",
    "    return fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
    "          'RandomForestRegressor': RandomForestRegressor(),\n",
    "          'Ridge': Ridge(),\n",
    "          'AdaBoostRegressor': AdaBoostRegressor(),\n",
    "          'LinearRegression': LinearRegression(),\n",
    "          'Lasso': Lasso(),\n",
    "          'ElasticNet': ElasticNet()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "fdiener = 'AdaBoostRegressor'\n",
    "fwho = 'RandomForestRegressor'\n",
    "parameters['AdaBoostRegressor'] = [{'loss': ['exponential'], 'n_estimators': [10], 'random_state': [42]}]\n",
    "parameters['RandomForestRegressor'] = [{'n_estimators': [20], 'max_depth': [3],\n",
    "                                        'min_samples_split': [2], 'min_samples_leaf': [1], 'max_features': ['auto'],\n",
    "                                         'random_state': [42]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostRegressor\n",
      "('exponential', 10, 42)\n",
      "Fitting estimator with 146 features.\n",
      "Fitting estimator with 145 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 141 features.\n",
      "Fitting estimator with 140 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 136 features.\n",
      "Fitting estimator with 135 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 146 features.\n",
      "Fitting estimator with 145 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 141 features.\n",
      "Fitting estimator with 140 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 136 features.\n",
      "Fitting estimator with 135 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 146 features.\n",
      "Fitting estimator with 145 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 141 features.\n",
      "Fitting estimator with 140 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 136 features.\n",
      "Fitting estimator with 135 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 146 features.\n",
      "Fitting estimator with 145 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 141 features.\n",
      "Fitting estimator with 140 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 136 features.\n",
      "Fitting estimator with 135 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 146 features.\n",
      "Fitting estimator with 145 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 141 features.\n",
      "Fitting estimator with 140 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 136 features.\n",
      "Fitting estimator with 135 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 146 features.\n",
      "Fitting estimator with 145 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 141 features.\n",
      "Fitting estimator with 140 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 136 features.\n",
      "Fitting estimator with 135 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 146 features.\n",
      "Fitting estimator with 145 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 141 features.\n",
      "Fitting estimator with 140 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 136 features.\n",
      "Fitting estimator with 135 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 146 features.\n",
      "Fitting estimator with 145 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 141 features.\n",
      "Fitting estimator with 140 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 136 features.\n",
      "Fitting estimator with 135 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 146 features.\n",
      "Fitting estimator with 145 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 141 features.\n",
      "Fitting estimator with 140 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 136 features.\n",
      "Fitting estimator with 135 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 146 features.\n",
      "Fitting estimator with 145 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 141 features.\n",
      "Fitting estimator with 140 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 136 features.\n",
      "Fitting estimator with 135 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 146 features.\n",
      "Fitting estimator with 145 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 141 features.\n",
      "Fitting estimator with 140 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 136 features.\n",
      "Fitting estimator with 135 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "28\n",
      "0.17017554392028786\n",
      "Dev:  0.17017554392028786 {'loss': 'exponential', 'n_estimators': 10, 'random_state': 42, 'fs': RFECV(cv=[[array([  0,   1,   2,   3,   4,   5,   6,   7,   9,  10,  11,  12,  13,\n",
      "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  26,  27,  28,\n",
      "        29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  42,\n",
      "        43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "        56,  57,  58,  59,  60,  62,  63,  64,  65,  67,  68,  69,  70,\n",
      "        71,  72,  73,  74,  75,  76,  77,  78,  80,  81,  82,  83,  84,\n",
      "        85,  86,  87,  88,  89,  90,  91,  92,  94,  95,  96,  97,  98,\n",
      "        99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 112, 113, 114,\n",
      "       115, 117, 118, 119, 1...\n",
      "           array([193, 454, 191, 302, 152, 407, 417,  96, 120, 321, 243, 405, 113,\n",
      "       293, 276, 474,  82, 232, 162, 222,  64, 357, 192, 365, 224, 280,\n",
      "        51, 251,  68, 413, 395, 341, 197,  20, 366, 301, 433, 230, 473,\n",
      "       429, 456, 310,  73, 323, 254,  85, 118, 398])]],\n",
      "      estimator=AdaBoostRegressor(loss='exponential', n_estimators=10,\n",
      "                                  random_state=42),\n",
      "      scoring=make_scorer(mean_absolute_error, greater_is_better=False),\n",
      "      verbose=1), 'fs_sup': 28}\n"
     ]
    }
   ],
   "source": [
    "#'Diener_11_500.0_1.0.txt'\n",
    "wb = 'Diener'\n",
    "dienerfn = './h-clusters0104/11_cos_500.0_1.0.txt'\n",
    "cldf = clusterdf(dienerfn)\n",
    "cldf['DF_' + wb.lower() + '_score'] = wdf2['DF_' + wb.lower() + '_score']\n",
    "data = {}\n",
    "data[wb] = cldf.dropna(subset = ['DF_' + wb.lower() + '_score'])\n",
    "fs = eval_nestedCV(data[wb].drop(['DF_' + wb.lower() + '_score'], axis=1), data[wb]['DF_' + wb.lower() + '_score'], wb, step=1, verbose=1, \n",
    "              method='RFECV', fsname='', funcname=fdiener, cvn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diener all: Diener_11_50.0_1.0.txt\n",
    "wb = 'Diener'\n",
    "dienerfn = './h-clusters0104/11_cos_50.0_1.0.txt'\n",
    "cldf = clusterdfgold(dienerfn)\n",
    "cldf.to_csv(gfoo+'besthall-clusters_Diener.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>16</th>\n",
       "      <th>19</th>\n",
       "      <th>...</th>\n",
       "      <th>596</th>\n",
       "      <th>607</th>\n",
       "      <th>614</th>\n",
       "      <th>630</th>\n",
       "      <th>631</th>\n",
       "      <th>633</th>\n",
       "      <th>638</th>\n",
       "      <th>651</th>\n",
       "      <th>665</th>\n",
       "      <th>695</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vk_id_encrypted</th>\n",
       "      <th>DF_freud_install_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137479231</th>\n",
       "      <th>2018-08-20 10:43:15</th>\n",
       "      <td>0.310291</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.003244</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.041454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180350141</th>\n",
       "      <th>2018-10-18 17:07:07</th>\n",
       "      <td>0.291562</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.042935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190703550</th>\n",
       "      <th>2018-09-14 12:38:34</th>\n",
       "      <td>0.253743</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.002569</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.044963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202633826</th>\n",
       "      <th>2018-09-23 02:38:48</th>\n",
       "      <td>0.292900</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.040813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333101115</th>\n",
       "      <th>2018-05-12 10:21:30</th>\n",
       "      <td>0.346461</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.003915</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.055219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693337702</th>\n",
       "      <th>2018-05-12 01:13:30</th>\n",
       "      <td>0.332011</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.049227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220279175</th>\n",
       "      <th>2018-08-25 01:44:05</th>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.045050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568464126</th>\n",
       "      <th>2018-08-14 02:36:17</th>\n",
       "      <td>0.340674</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.052648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815532206</th>\n",
       "      <th>2018-10-25 14:25:41</th>\n",
       "      <td>0.280562</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.002684</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.004592</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.050510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383753406</th>\n",
       "      <th>2018-07-01 13:07:43</th>\n",
       "      <td>0.290663</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.037664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>372 rows  146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            1         2         3         4    \\\n",
       "vk_id_encrypted DF_freud_install_time                                           \n",
       "137479231       2018-08-20 10:43:15    0.310291  0.000154  0.003244  0.000474   \n",
       "180350141       2018-10-18 17:07:07    0.291562  0.000113  0.001616  0.000430   \n",
       "190703550       2018-09-14 12:38:34    0.253743  0.000165  0.003113  0.000638   \n",
       "202633826       2018-09-23 02:38:48    0.292900  0.000172  0.001518  0.000276   \n",
       "333101115       2018-05-12 10:21:30    0.346461  0.000410  0.002440  0.000410   \n",
       "...                                         ...       ...       ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.332011  0.000115  0.001976  0.000331   \n",
       "220279175       2018-08-25 01:44:05    0.331522  0.000092  0.001240  0.000291   \n",
       "568464126       2018-08-14 02:36:17    0.340674  0.000168  0.002150  0.000274   \n",
       "815532206       2018-10-25 14:25:41    0.280562  0.000071  0.002684  0.000247   \n",
       "383753406       2018-07-01 13:07:43    0.290663  0.000257  0.001234  0.000240   \n",
       "\n",
       "                                            5         6         9         10   \\\n",
       "vk_id_encrypted DF_freud_install_time                                           \n",
       "137479231       2018-08-20 10:43:15    0.000072  0.000391  0.001947  0.000216   \n",
       "180350141       2018-10-18 17:07:07    0.000219  0.000384  0.000457  0.000788   \n",
       "190703550       2018-09-14 12:38:34    0.000205  0.000307  0.002569  0.000134   \n",
       "202633826       2018-09-23 02:38:48    0.000103  0.000172  0.001483  0.000310   \n",
       "333101115       2018-05-12 10:21:30    0.000455  0.000510  0.003915  0.000166   \n",
       "...                                         ...       ...       ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.000027  0.000277  0.001646  0.000054   \n",
       "220279175       2018-08-25 01:44:05    0.000131  0.001041  0.000321  0.000137   \n",
       "568464126       2018-08-14 02:36:17    0.000162  0.000347  0.001433  0.000045   \n",
       "815532206       2018-10-25 14:25:41    0.000035  0.000247  0.004592  0.000071   \n",
       "383753406       2018-07-01 13:07:43    0.000111  0.000188  0.001148  0.000043   \n",
       "\n",
       "                                            16        19   ...       596  \\\n",
       "vk_id_encrypted DF_freud_install_time                      ...             \n",
       "137479231       2018-08-20 10:43:15    0.000484  0.000762  ...  0.000124   \n",
       "180350141       2018-10-18 17:07:07    0.000179  0.001066  ...  0.000073   \n",
       "190703550       2018-09-14 12:38:34    0.000150  0.001206  ...  0.000102   \n",
       "202633826       2018-09-23 02:38:48    0.000069  0.002139  ...  0.000069   \n",
       "333101115       2018-05-12 10:21:30    0.000466  0.001675  ...  0.000078   \n",
       "...                                         ...       ...  ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.000229  0.001369  ...  0.000121   \n",
       "220279175       2018-08-25 01:44:05    0.000146  0.000940  ...  0.000128   \n",
       "568464126       2018-08-14 02:36:17    0.000291  0.001226  ...  0.000067   \n",
       "815532206       2018-10-25 14:25:41    0.000742  0.001236  ...  0.000071   \n",
       "383753406       2018-07-01 13:07:43    0.000231  0.000857  ...  0.000043   \n",
       "\n",
       "                                            607       614       630       631  \\\n",
       "vk_id_encrypted DF_freud_install_time                                           \n",
       "137479231       2018-08-20 10:43:15    0.000144  0.000103  0.000113  0.000021   \n",
       "180350141       2018-10-18 17:07:07    0.000086  0.000079  0.000119  0.000033   \n",
       "190703550       2018-09-14 12:38:34    0.000158  0.000102  0.000118  0.000173   \n",
       "202633826       2018-09-23 02:38:48    0.000034  0.000103  0.000034  0.000000   \n",
       "333101115       2018-05-12 10:21:30    0.000067  0.000100  0.000122  0.000111   \n",
       "...                                         ...       ...       ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.000027  0.000074  0.000148  0.000040   \n",
       "220279175       2018-08-25 01:44:05    0.000036  0.000059  0.000074  0.000059   \n",
       "568464126       2018-08-14 02:36:17    0.000101  0.000207  0.000118  0.000022   \n",
       "815532206       2018-10-25 14:25:41    0.000000  0.000071  0.000177  0.000035   \n",
       "383753406       2018-07-01 13:07:43    0.000137  0.000086  0.000257  0.000060   \n",
       "\n",
       "                                            633       638       651       665  \\\n",
       "vk_id_encrypted DF_freud_install_time                                           \n",
       "137479231       2018-08-20 10:43:15    0.000031  0.000257  0.000185  0.000371   \n",
       "180350141       2018-10-18 17:07:07    0.000093  0.000093  0.000060  0.000358   \n",
       "190703550       2018-09-14 12:38:34    0.000087  0.000158  0.000095  0.000386   \n",
       "202633826       2018-09-23 02:38:48    0.000034  0.000172  0.000069  0.000138   \n",
       "333101115       2018-05-12 10:21:30    0.000166  0.000399  0.000144  0.000266   \n",
       "...                                         ...       ...       ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.000074  0.000175  0.000115  0.000499   \n",
       "220279175       2018-08-25 01:44:05    0.000059  0.000042  0.000048  0.000294   \n",
       "568464126       2018-08-14 02:36:17    0.000202  0.000140  0.000056  0.000263   \n",
       "815532206       2018-10-25 14:25:41    0.000000  0.000177  0.000141  0.000353   \n",
       "383753406       2018-07-01 13:07:43    0.000094  0.000111  0.000077  0.000103   \n",
       "\n",
       "                                            695  \n",
       "vk_id_encrypted DF_freud_install_time            \n",
       "137479231       2018-08-20 10:43:15    0.041454  \n",
       "180350141       2018-10-18 17:07:07    0.042935  \n",
       "190703550       2018-09-14 12:38:34    0.044963  \n",
       "202633826       2018-09-23 02:38:48    0.040813  \n",
       "333101115       2018-05-12 10:21:30    0.055219  \n",
       "...                                         ...  \n",
       "693337702       2018-05-12 01:13:30    0.049227  \n",
       "220279175       2018-08-25 01:44:05    0.045050  \n",
       "568464126       2018-08-14 02:36:17    0.052648  \n",
       "815532206       2018-10-25 14:25:41    0.050510  \n",
       "383753406       2018-07-01 13:07:43    0.037664  \n",
       "\n",
       "[372 rows x 146 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#WHO all: WHO_11_500.0_1.0VERYRARE.txt\n",
    "wb = 'WHO'\n",
    "dienerfn = './h-clusters0104/11_cos_500.0_1.0VERYRARE.txt'\n",
    "cldf = clusterdfgold(dienerfn)\n",
    "cldf.to_csv(gfoo+'besthall-clusters_WHO.csv')\n",
    "cldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor\n",
      "(3, 'auto', 1, 2, 20, 42)\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "19\n",
      "0.1525534500385621\n",
      "Dev:  0.1525534500385621 {'max_depth': 3, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 20, 'random_state': 42, 'fs': RFECV(cv=[[array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  17,  18,  19,  20,  21,  22,  24,  25,  27,  28,\n",
      "        29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "        43,  44,  45,  46,  47,  48,  49,  50,  52,  53,  54,  55,  56,\n",
      "        57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  69,  70,\n",
      "        71,  73,  74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  85,\n",
      "        86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n",
      "        99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "       112, 113, 114, 115, 11...\n",
      "       477, 478, 479, 480, 481, 482, 483, 484, 485]),\n",
      "           array([379, 261, 416,  99, 143, 386, 444,  46, 467, 355, 159, 239, 284,\n",
      "       114, 233, 290, 432, 394,  39, 309, 107, 115,  41, 237, 192, 377,\n",
      "       415, 368,  11, 373, 214,  67, 307, 135, 342, 296, 455, 218, 187,\n",
      "       431,  83, 212, 263, 219, 358, 228, 395, 241])]],\n",
      "      estimator=RandomForestRegressor(max_depth=3, n_estimators=20,\n",
      "                                      random_state=42),\n",
      "      scoring=make_scorer(mean_absolute_error, greater_is_better=False),\n",
      "      verbose=1), 'fs_sup': 19}\n"
     ]
    }
   ],
   "source": [
    "#'WHO_11_1.0_0.0VERYRARE.txt'\n",
    "wb = 'WHO'\n",
    "whofn = './h-clusters0104/11_cos_1.0_0.0VERYRARE.txt'\n",
    "cldf = clusterdf(whofn)\n",
    "cldf['DF_' + wb.lower() + '_score'] = wdf2['DF_' + wb.lower() + '_score']\n",
    "data[wb] = cldf.dropna(subset = ['DF_' + wb.lower() + '_score'])\n",
    "fs = eval_nestedCV(data[wb].drop(['DF_' + wb.lower() + '_score'], axis=1), data[wb]['DF_' + wb.lower() + '_score'], wb, step=1, verbose=1, \n",
    "              method='RFECV', fsname='', funcname=fwho, cvn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134, 19)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fs.get_support()), sum(fs.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([  1,  12,  14,  22,  50,  68,  85, 120, 132, 200, 334, 372, 374, 381,\n",
       "       465, 495, 567, 610, 639],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = data[wb].drop(['DF_' + wb.lower() + '_score'], axis=1).columns[fs.get_support()]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwdf = pd.read_csv('word-matrix-1year_mindf10_minwords20.csv', index_col=0)\n",
    "g1 = pd.read_csv(gfoo+'traincorr1.csv', index_col=[0, 1])\n",
    "g2 = pd.read_csv(gfoo+'testcorr1.csv', index_col=[0,1])\n",
    "g = pd.concat([g1, g2])\n",
    "allwdf = g.join(allwdf)[allwdf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterdfgold(fn):\n",
    "    cdf = pd.read_csv(fn, index_col = 0)\n",
    "    resd, centerd = {}, {}\n",
    "    ids, centers = [], []\n",
    "    for i, r in cdf.iterrows():\n",
    "        #if rare or (r.WHO_P < 0.05) or (r.Diener_P < 0.05):\n",
    "            ws = r.ex.strip().split(', ')\n",
    "            for w in ws:\n",
    "                resd[w] = i\n",
    "            #if rare:\n",
    "            #    ids.append(i)\n",
    "            #    centers.append(np.mean([model[w] for w in ws], axis=0))\n",
    "                \n",
    "            #print(i)\n",
    "    cls = np.asarray([resd.get(w, -1) for w in allwdf.columns])\n",
    "    cldf = allwdf.groupby(by = cls, axis = 'columns').sum()\n",
    "    #print(cldf.shape)\n",
    "    cldf.drop(-1, axis=1, inplace = True)\n",
    "    return cldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>16</th>\n",
       "      <th>19</th>\n",
       "      <th>...</th>\n",
       "      <th>596</th>\n",
       "      <th>607</th>\n",
       "      <th>614</th>\n",
       "      <th>630</th>\n",
       "      <th>631</th>\n",
       "      <th>633</th>\n",
       "      <th>638</th>\n",
       "      <th>651</th>\n",
       "      <th>665</th>\n",
       "      <th>695</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vk_id_encrypted</th>\n",
       "      <th>DF_freud_install_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137479231</th>\n",
       "      <th>2018-08-20 10:43:15</th>\n",
       "      <td>0.304482</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.001936</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.037427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180350141</th>\n",
       "      <th>2018-10-18 17:07:07</th>\n",
       "      <td>0.286867</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.040352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190703550</th>\n",
       "      <th>2018-09-14 12:38:34</th>\n",
       "      <td>0.248195</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.041354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202633826</th>\n",
       "      <th>2018-09-23 02:38:48</th>\n",
       "      <td>0.287346</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.039847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333101115</th>\n",
       "      <th>2018-05-12 10:21:30</th>\n",
       "      <td>0.340361</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.003893</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.051548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693337702</th>\n",
       "      <th>2018-05-12 01:13:30</th>\n",
       "      <td>0.328504</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.047629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220279175</th>\n",
       "      <th>2018-08-25 01:44:05</th>\n",
       "      <td>0.328664</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.043399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568464126</th>\n",
       "      <th>2018-08-14 02:36:17</th>\n",
       "      <td>0.334158</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.001371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.050756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815532206</th>\n",
       "      <th>2018-10-25 14:25:41</th>\n",
       "      <td>0.276783</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.004557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.047932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383753406</th>\n",
       "      <th>2018-07-01 13:07:43</th>\n",
       "      <td>0.285779</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.035848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>372 rows  146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            1         2         3         4    \\\n",
       "vk_id_encrypted DF_freud_install_time                                           \n",
       "137479231       2018-08-20 10:43:15    0.304482  0.000031  0.002863  0.000185   \n",
       "180350141       2018-10-18 17:07:07    0.286867  0.000033  0.001470  0.000119   \n",
       "190703550       2018-09-14 12:38:34    0.248195  0.000008  0.002900  0.000173   \n",
       "202633826       2018-09-23 02:38:48    0.287346  0.000172  0.001276  0.000207   \n",
       "333101115       2018-05-12 10:21:30    0.340361  0.000089  0.002096  0.000133   \n",
       "...                                         ...       ...       ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.328504  0.000054  0.001895  0.000094   \n",
       "220279175       2018-08-25 01:44:05    0.328664  0.000027  0.001127  0.000077   \n",
       "568464126       2018-08-14 02:36:17    0.334158  0.000022  0.001858  0.000101   \n",
       "815532206       2018-10-25 14:25:41    0.276783  0.000035  0.002473  0.000035   \n",
       "383753406       2018-07-01 13:07:43    0.285779  0.000069  0.001191  0.000069   \n",
       "\n",
       "                                            5         6         9         10   \\\n",
       "vk_id_encrypted DF_freud_install_time                                           \n",
       "137479231       2018-08-20 10:43:15    0.000000  0.000124  0.001936  0.000000   \n",
       "180350141       2018-10-18 17:07:07    0.000113  0.000212  0.000238  0.000179   \n",
       "190703550       2018-09-14 12:38:34    0.000047  0.000095  0.002151  0.000008   \n",
       "202633826       2018-09-23 02:38:48    0.000069  0.000000  0.001276  0.000103   \n",
       "333101115       2018-05-12 10:21:30    0.000233  0.000189  0.003893  0.000067   \n",
       "...                                         ...       ...       ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.000013  0.000067  0.001632  0.000007   \n",
       "220279175       2018-08-25 01:44:05    0.000039  0.000874  0.000306  0.000021   \n",
       "568464126       2018-08-14 02:36:17    0.000084  0.000213  0.001371  0.000000   \n",
       "815532206       2018-10-25 14:25:41    0.000000  0.000106  0.004557  0.000000   \n",
       "383753406       2018-07-01 13:07:43    0.000026  0.000120  0.000977  0.000000   \n",
       "\n",
       "                                            16        19   ...       596  \\\n",
       "vk_id_encrypted DF_freud_install_time                      ...             \n",
       "137479231       2018-08-20 10:43:15    0.000041  0.000474  ...  0.000103   \n",
       "180350141       2018-10-18 17:07:07    0.000013  0.000662  ...  0.000046   \n",
       "190703550       2018-09-14 12:38:34    0.000008  0.000764  ...  0.000071   \n",
       "202633826       2018-09-23 02:38:48    0.000069  0.001587  ...  0.000069   \n",
       "333101115       2018-05-12 10:21:30    0.000033  0.001065  ...  0.000055   \n",
       "...                                         ...       ...  ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.000013  0.001025  ...  0.000088   \n",
       "220279175       2018-08-25 01:44:05    0.000009  0.000809  ...  0.000125   \n",
       "568464126       2018-08-14 02:36:17    0.000011  0.000929  ...  0.000050   \n",
       "815532206       2018-10-25 14:25:41    0.000071  0.000954  ...  0.000035   \n",
       "383753406       2018-07-01 13:07:43    0.000034  0.000548  ...  0.000026   \n",
       "\n",
       "                                            607       614       630       631  \\\n",
       "vk_id_encrypted DF_freud_install_time                                           \n",
       "137479231       2018-08-20 10:43:15    0.000021  0.000010  0.000103  0.000010   \n",
       "180350141       2018-10-18 17:07:07    0.000020  0.000013  0.000093  0.000033   \n",
       "190703550       2018-09-14 12:38:34    0.000008  0.000000  0.000118  0.000126   \n",
       "202633826       2018-09-23 02:38:48    0.000000  0.000000  0.000034  0.000000   \n",
       "333101115       2018-05-12 10:21:30    0.000022  0.000011  0.000122  0.000100   \n",
       "...                                         ...       ...       ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.000027  0.000013  0.000148  0.000040   \n",
       "220279175       2018-08-25 01:44:05    0.000018  0.000009  0.000074  0.000051   \n",
       "568464126       2018-08-14 02:36:17    0.000006  0.000028  0.000112  0.000017   \n",
       "815532206       2018-10-25 14:25:41    0.000000  0.000000  0.000177  0.000035   \n",
       "383753406       2018-07-01 13:07:43    0.000017  0.000009  0.000240  0.000060   \n",
       "\n",
       "                                            633       638       651       665  \\\n",
       "vk_id_encrypted DF_freud_install_time                                           \n",
       "137479231       2018-08-20 10:43:15    0.000000  0.000000  0.000082  0.000299   \n",
       "180350141       2018-10-18 17:07:07    0.000026  0.000007  0.000033  0.000344   \n",
       "190703550       2018-09-14 12:38:34    0.000047  0.000008  0.000016  0.000355   \n",
       "202633826       2018-09-23 02:38:48    0.000034  0.000000  0.000069  0.000138   \n",
       "333101115       2018-05-12 10:21:30    0.000000  0.000100  0.000055  0.000211   \n",
       "...                                         ...       ...       ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.000013  0.000047  0.000000  0.000499   \n",
       "220279175       2018-08-25 01:44:05    0.000012  0.000012  0.000015  0.000271   \n",
       "568464126       2018-08-14 02:36:17    0.000050  0.000045  0.000017  0.000218   \n",
       "815532206       2018-10-25 14:25:41    0.000000  0.000000  0.000106  0.000318   \n",
       "383753406       2018-07-01 13:07:43    0.000000  0.000034  0.000026  0.000086   \n",
       "\n",
       "                                            695  \n",
       "vk_id_encrypted DF_freud_install_time            \n",
       "137479231       2018-08-20 10:43:15    0.037427  \n",
       "180350141       2018-10-18 17:07:07    0.040352  \n",
       "190703550       2018-09-14 12:38:34    0.041354  \n",
       "202633826       2018-09-23 02:38:48    0.039847  \n",
       "333101115       2018-05-12 10:21:30    0.051548  \n",
       "...                                         ...  \n",
       "693337702       2018-05-12 01:13:30    0.047629  \n",
       "220279175       2018-08-25 01:44:05    0.043399  \n",
       "568464126       2018-08-14 02:36:17    0.050756  \n",
       "815532206       2018-10-25 14:25:41    0.047932  \n",
       "383753406       2018-07-01 13:07:43    0.035848  \n",
       "\n",
       "[372 rows x 146 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dienercldf = clusterdfgold(dienerfn)\n",
    "dienercldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>14</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>22</th>\n",
       "      <th>28</th>\n",
       "      <th>...</th>\n",
       "      <th>584</th>\n",
       "      <th>593</th>\n",
       "      <th>607</th>\n",
       "      <th>609</th>\n",
       "      <th>610</th>\n",
       "      <th>616</th>\n",
       "      <th>639</th>\n",
       "      <th>647</th>\n",
       "      <th>659</th>\n",
       "      <th>667</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vk_id_encrypted</th>\n",
       "      <th>DF_freud_install_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137479231</th>\n",
       "      <th>2018-08-20 10:43:15</th>\n",
       "      <td>0.316470</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.038055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180350141</th>\n",
       "      <th>2018-10-18 17:07:07</th>\n",
       "      <td>0.299827</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.033756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190703550</th>\n",
       "      <th>2018-09-14 12:38:34</th>\n",
       "      <td>0.267525</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.038068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202633826</th>\n",
       "      <th>2018-09-23 02:38:48</th>\n",
       "      <td>0.311564</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.033395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333101115</th>\n",
       "      <th>2018-05-12 10:21:30</th>\n",
       "      <td>0.363052</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.043662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693337702</th>\n",
       "      <th>2018-05-12 01:13:30</th>\n",
       "      <td>0.343101</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.038914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220279175</th>\n",
       "      <th>2018-08-25 01:44:05</th>\n",
       "      <td>0.342663</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.038617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568464126</th>\n",
       "      <th>2018-08-14 02:36:17</th>\n",
       "      <td>0.354154</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.041480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815532206</th>\n",
       "      <th>2018-10-25 14:25:41</th>\n",
       "      <td>0.298400</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.039914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383753406</th>\n",
       "      <th>2018-07-01 13:07:43</th>\n",
       "      <td>0.299128</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.032035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>372 rows  134 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            1         6         7         11   \\\n",
       "vk_id_encrypted DF_freud_install_time                                           \n",
       "137479231       2018-08-20 10:43:15    0.316470  0.000278  0.000731  0.000268   \n",
       "180350141       2018-10-18 17:07:07    0.299827  0.000450  0.001748  0.000298   \n",
       "190703550       2018-09-14 12:38:34    0.267525  0.000150  0.000607  0.000244   \n",
       "202633826       2018-09-23 02:38:48    0.311564  0.000138  0.000276  0.000138   \n",
       "333101115       2018-05-12 10:21:30    0.363052  0.000166  0.000599  0.000466   \n",
       "...                                         ...       ...       ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.343101  0.000108  0.000391  0.000074   \n",
       "220279175       2018-08-25 01:44:05    0.342663  0.000134  0.000681  0.000155   \n",
       "568464126       2018-08-14 02:36:17    0.354154  0.000078  0.000297  0.000291   \n",
       "815532206       2018-10-25 14:25:41    0.298400  0.000106  0.000389  0.000247   \n",
       "383753406       2018-07-01 13:07:43    0.299128  0.000077  0.000377  0.000728   \n",
       "\n",
       "                                            12        14        17        18   \\\n",
       "vk_id_encrypted DF_freud_install_time                                           \n",
       "137479231       2018-08-20 10:43:15    0.000711  0.000031  0.000113  0.000165   \n",
       "180350141       2018-10-18 17:07:07    0.000132  0.000106  0.000146  0.000046   \n",
       "190703550       2018-09-14 12:38:34    0.000158  0.000087  0.000465  0.000087   \n",
       "202633826       2018-09-23 02:38:48    0.000207  0.000138  0.000345  0.000000   \n",
       "333101115       2018-05-12 10:21:30    0.000399  0.000244  0.000200  0.000166   \n",
       "...                                         ...       ...       ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.000101  0.000155  0.000135  0.000054   \n",
       "220279175       2018-08-25 01:44:05    0.000125  0.000042  0.000083  0.000036   \n",
       "568464126       2018-08-14 02:36:17    0.000140  0.000146  0.000162  0.000062   \n",
       "815532206       2018-10-25 14:25:41    0.000459  0.000000  0.000000  0.000071   \n",
       "383753406       2018-07-01 13:07:43    0.000103  0.000051  0.000188  0.000077   \n",
       "\n",
       "                                            22        28   ...       584  \\\n",
       "vk_id_encrypted DF_freud_install_time                      ...             \n",
       "137479231       2018-08-20 10:43:15    0.000072  0.000175  ...  0.000165   \n",
       "180350141       2018-10-18 17:07:07    0.000199  0.000060  ...  0.000079   \n",
       "190703550       2018-09-14 12:38:34    0.000772  0.000489  ...  0.000158   \n",
       "202633826       2018-09-23 02:38:48    0.000586  0.000034  ...  0.000000   \n",
       "333101115       2018-05-12 10:21:30    0.000067  0.000111  ...  0.000089   \n",
       "...                                         ...       ...  ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.000121  0.000074  ...  0.000027   \n",
       "220279175       2018-08-25 01:44:05    0.000175  0.000089  ...  0.000036   \n",
       "568464126       2018-08-14 02:36:17    0.000162  0.000123  ...  0.000084   \n",
       "815532206       2018-10-25 14:25:41    0.000035  0.000000  ...  0.000000   \n",
       "383753406       2018-07-01 13:07:43    0.000137  0.000137  ...  0.000077   \n",
       "\n",
       "                                            593       607       609       610  \\\n",
       "vk_id_encrypted DF_freud_install_time                                           \n",
       "137479231       2018-08-20 10:43:15    0.000185  0.000113  0.000824  0.000031   \n",
       "180350141       2018-10-18 17:07:07    0.000066  0.000119  0.000437  0.000046   \n",
       "190703550       2018-09-14 12:38:34    0.000095  0.000118  0.000512  0.000213   \n",
       "202633826       2018-09-23 02:38:48    0.000241  0.000034  0.000759  0.000000   \n",
       "333101115       2018-05-12 10:21:30    0.000233  0.000122  0.000876  0.000144   \n",
       "...                                         ...       ...       ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.000128  0.000148  0.000337  0.000040   \n",
       "220279175       2018-08-25 01:44:05    0.000095  0.000074  0.000357  0.000101   \n",
       "568464126       2018-08-14 02:36:17    0.000162  0.000112  0.000476  0.000034   \n",
       "815532206       2018-10-25 14:25:41    0.000035  0.000177  0.000141  0.000035   \n",
       "383753406       2018-07-01 13:07:43    0.000137  0.000257  0.000728  0.000069   \n",
       "\n",
       "                                            616       639       647       659  \\\n",
       "vk_id_encrypted DF_freud_install_time                                           \n",
       "137479231       2018-08-20 10:43:15    0.000144  0.000062  0.000309  0.000566   \n",
       "180350141       2018-10-18 17:07:07    0.000093  0.000079  0.000199  0.000126   \n",
       "190703550       2018-09-14 12:38:34    0.000110  0.000189  0.000276  0.000158   \n",
       "202633826       2018-09-23 02:38:48    0.000000  0.000103  0.000103  0.000172   \n",
       "333101115       2018-05-12 10:21:30    0.000355  0.000266  0.000311  0.000388   \n",
       "...                                         ...       ...       ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.000155  0.000054  0.000121  0.000236   \n",
       "220279175       2018-08-25 01:44:05    0.000033  0.000092  0.000125  0.000065   \n",
       "568464126       2018-08-14 02:36:17    0.000151  0.000560  0.000157  0.000297   \n",
       "815532206       2018-10-25 14:25:41    0.000247  0.000000  0.000035  0.000071   \n",
       "383753406       2018-07-01 13:07:43    0.000077  0.000103  0.000171  0.000146   \n",
       "\n",
       "                                            667  \n",
       "vk_id_encrypted DF_freud_install_time            \n",
       "137479231       2018-08-20 10:43:15    0.038055  \n",
       "180350141       2018-10-18 17:07:07    0.033756  \n",
       "190703550       2018-09-14 12:38:34    0.038068  \n",
       "202633826       2018-09-23 02:38:48    0.033395  \n",
       "333101115       2018-05-12 10:21:30    0.043662  \n",
       "...                                         ...  \n",
       "693337702       2018-05-12 01:13:30    0.038914  \n",
       "220279175       2018-08-25 01:44:05    0.038617  \n",
       "568464126       2018-08-14 02:36:17    0.041480  \n",
       "815532206       2018-10-25 14:25:41    0.039914  \n",
       "383753406       2018-07-01 13:07:43    0.032035  \n",
       "\n",
       "[372 rows x 134 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whocldf = clusterdfgold(whofn)\n",
    "whocldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dienercldf[cols].to_csv(gfoo+'besth-clusters_Diener.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "whocldf[cols].to_csv(gfoo+'besth-clusters_WHO.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>12</th>\n",
       "      <th>14</th>\n",
       "      <th>22</th>\n",
       "      <th>50</th>\n",
       "      <th>68</th>\n",
       "      <th>85</th>\n",
       "      <th>120</th>\n",
       "      <th>132</th>\n",
       "      <th>200</th>\n",
       "      <th>334</th>\n",
       "      <th>372</th>\n",
       "      <th>374</th>\n",
       "      <th>381</th>\n",
       "      <th>465</th>\n",
       "      <th>495</th>\n",
       "      <th>567</th>\n",
       "      <th>610</th>\n",
       "      <th>639</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vk_id_encrypted</th>\n",
       "      <th>DF_freud_install_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137479231</th>\n",
       "      <th>2018-08-20 10:43:15</th>\n",
       "      <td>0.316470</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180350141</th>\n",
       "      <th>2018-10-18 17:07:07</th>\n",
       "      <td>0.299827</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190703550</th>\n",
       "      <th>2018-09-14 12:38:34</th>\n",
       "      <td>0.267525</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202633826</th>\n",
       "      <th>2018-09-23 02:38:48</th>\n",
       "      <td>0.311564</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333101115</th>\n",
       "      <th>2018-05-12 10:21:30</th>\n",
       "      <td>0.363052</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693337702</th>\n",
       "      <th>2018-05-12 01:13:30</th>\n",
       "      <td>0.343101</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220279175</th>\n",
       "      <th>2018-08-25 01:44:05</th>\n",
       "      <td>0.342663</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568464126</th>\n",
       "      <th>2018-08-14 02:36:17</th>\n",
       "      <td>0.354154</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815532206</th>\n",
       "      <th>2018-10-25 14:25:41</th>\n",
       "      <td>0.298400</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383753406</th>\n",
       "      <th>2018-07-01 13:07:43</th>\n",
       "      <td>0.299128</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>372 rows  19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            1         12        14        22   \\\n",
       "vk_id_encrypted DF_freud_install_time                                           \n",
       "137479231       2018-08-20 10:43:15    0.316470  0.000711  0.000031  0.000072   \n",
       "180350141       2018-10-18 17:07:07    0.299827  0.000132  0.000106  0.000199   \n",
       "190703550       2018-09-14 12:38:34    0.267525  0.000158  0.000087  0.000772   \n",
       "202633826       2018-09-23 02:38:48    0.311564  0.000207  0.000138  0.000586   \n",
       "333101115       2018-05-12 10:21:30    0.363052  0.000399  0.000244  0.000067   \n",
       "...                                         ...       ...       ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.343101  0.000101  0.000155  0.000121   \n",
       "220279175       2018-08-25 01:44:05    0.342663  0.000125  0.000042  0.000175   \n",
       "568464126       2018-08-14 02:36:17    0.354154  0.000140  0.000146  0.000162   \n",
       "815532206       2018-10-25 14:25:41    0.298400  0.000459  0.000000  0.000035   \n",
       "383753406       2018-07-01 13:07:43    0.299128  0.000103  0.000051  0.000137   \n",
       "\n",
       "                                            50        68        85        120  \\\n",
       "vk_id_encrypted DF_freud_install_time                                           \n",
       "137479231       2018-08-20 10:43:15    0.000082  0.001071  0.000165  0.000257   \n",
       "180350141       2018-10-18 17:07:07    0.000060  0.000722  0.000146  0.000086   \n",
       "190703550       2018-09-14 12:38:34    0.000079  0.000441  0.000110  0.000189   \n",
       "202633826       2018-09-23 02:38:48    0.000000  0.000517  0.000069  0.000103   \n",
       "333101115       2018-05-12 10:21:30    0.000055  0.000455  0.000189  0.000377   \n",
       "...                                         ...       ...       ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.000061  0.000533  0.000074  0.000175   \n",
       "220279175       2018-08-25 01:44:05    0.000042  0.000491  0.000074  0.000244   \n",
       "568464126       2018-08-14 02:36:17    0.000050  0.000672  0.000067  0.000313   \n",
       "815532206       2018-10-25 14:25:41    0.000106  0.000883  0.000177  0.000212   \n",
       "383753406       2018-07-01 13:07:43    0.000043  0.000685  0.000137  0.000240   \n",
       "\n",
       "                                            132       200       334       372  \\\n",
       "vk_id_encrypted DF_freud_install_time                                           \n",
       "137479231       2018-08-20 10:43:15    0.000721  0.000762  0.000278  0.000484   \n",
       "180350141       2018-10-18 17:07:07    0.000742  0.001298  0.000086  0.000245   \n",
       "190703550       2018-09-14 12:38:34    0.000859  0.000796  0.000165  0.000559   \n",
       "202633826       2018-09-23 02:38:48    0.000345  0.000310  0.000034  0.000069   \n",
       "333101115       2018-05-12 10:21:30    0.000665  0.000887  0.000166  0.000233   \n",
       "...                                         ...       ...       ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.000648  0.000614  0.000040  0.000155   \n",
       "220279175       2018-08-25 01:44:05    0.000434  0.000467  0.000042  0.000202   \n",
       "568464126       2018-08-14 02:36:17    0.000403  0.000655  0.000084  0.000174   \n",
       "815532206       2018-10-25 14:25:41    0.000495  0.000530  0.000106  0.000177   \n",
       "383753406       2018-07-01 13:07:43    0.000608  0.000471  0.000060  0.000129   \n",
       "\n",
       "                                            374       381       465       495  \\\n",
       "vk_id_encrypted DF_freud_install_time                                           \n",
       "137479231       2018-08-20 10:43:15    0.000330  0.000288  0.000165  0.000134   \n",
       "180350141       2018-10-18 17:07:07    0.000159  0.000404  0.000132  0.000040   \n",
       "190703550       2018-09-14 12:38:34    0.000213  0.000213  0.000197  0.000063   \n",
       "202633826       2018-09-23 02:38:48    0.000103  0.000172  0.000207  0.000000   \n",
       "333101115       2018-05-12 10:21:30    0.000311  0.000266  0.000299  0.000111   \n",
       "...                                         ...       ...       ...       ...   \n",
       "693337702       2018-05-12 01:13:30    0.000067  0.000007  0.000074  0.000067   \n",
       "220279175       2018-08-25 01:44:05    0.000089  0.000059  0.000098  0.000054   \n",
       "568464126       2018-08-14 02:36:17    0.000168  0.000202  0.000509  0.000039   \n",
       "815532206       2018-10-25 14:25:41    0.000424  0.000071  0.000106  0.000106   \n",
       "383753406       2018-07-01 13:07:43    0.000146  0.000171  0.000163  0.000060   \n",
       "\n",
       "                                            567       610       639  \n",
       "vk_id_encrypted DF_freud_install_time                                \n",
       "137479231       2018-08-20 10:43:15    0.000134  0.000031  0.000062  \n",
       "180350141       2018-10-18 17:07:07    0.000119  0.000046  0.000079  \n",
       "190703550       2018-09-14 12:38:34    0.000173  0.000213  0.000189  \n",
       "202633826       2018-09-23 02:38:48    0.000069  0.000000  0.000103  \n",
       "333101115       2018-05-12 10:21:30    0.000155  0.000144  0.000266  \n",
       "...                                         ...       ...       ...  \n",
       "693337702       2018-05-12 01:13:30    0.000175  0.000040  0.000054  \n",
       "220279175       2018-08-25 01:44:05    0.000068  0.000101  0.000092  \n",
       "568464126       2018-08-14 02:36:17    0.000090  0.000034  0.000560  \n",
       "815532206       2018-10-25 14:25:41    0.000000  0.000035  0.000000  \n",
       "383753406       2018-07-01 13:07:43    0.000454  0.000069  0.000103  \n",
       "\n",
       "[372 rows x 19 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whocldf[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vieweval(wb, rfoo = './res_april2021/', thresh = 0.33):\n",
    "    i = 0\n",
    "    resd = {}\n",
    "    for fn in os.listdir(rfoo):\n",
    "        if fn.startswith('final') and ('testscore' in fn) and (wb in fn):\n",
    "            #print(fn)\n",
    "            f = open(rfoo+fn, 'r')\n",
    "            for line in f:\n",
    "                if line.startswith('method'):\n",
    "                    continue\n",
    "                line = line.strip()\n",
    "                l = line.split('\\t')\n",
    "                resd[l[2]] = resd.get(l[2], [])\n",
    "                resd[l[2]].append(l + [fn])\n",
    "                #print(l[-1])\n",
    "                if float(l[-2]) > thresh:\n",
    "                    i+=1\n",
    "                    print(str(i)+'.', fn, line.strip())\n",
    "            f.close()\n",
    "    for k,val in resd.items():\n",
    "        resd[k].sort(key = lambda x: float(x[-4]))\n",
    "        print(k, len(val))\n",
    "        for v in val:\n",
    "            print(' '.join(v))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. final_reg4_testscores_max_Lasso_WHO_RFECV.tsv RFECV\twho\tAppCats + Behavior + Words\tLasso\t0.1369\t0.1464\t0.3636\t0.074\n",
      "2. final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv RFECV\twho\tWords\tElasticNet\t0.1382\t0.1456\t0.354\t0.0864\n",
      "3. final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv RFECV\twho\tAppCats + Words\tElasticNet\t0.1345\t0.1456\t0.3581\t0.0995\n",
      "4. final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv RFECV\twho\tRuLIWC + Words\tElasticNet\t0.1394\t0.1438\t0.3729\t0.1058\n",
      "5. final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv RFECV\twho\tAppCats + Behavior + Words\tElasticNet\t0.1326\t0.1467\t0.3316\t0.0731\n",
      "6. final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv RFECV\twho\tAppCats + RuLIWC + Behavior + Words\tElasticNet\t0.1325\t0.1461\t0.3592\t0.0999\n",
      "7. final_clust_reg4_testscores_max_Lasso_WHO_RFECV.tsv RFECV\twho\tclusters + AppCats + Behavior + Words\tLasso\t0.1373\t0.1457\t0.3339\t0.0701\n",
      "8. final_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv RFECV\twho\tclusters + Words\tElasticNet\t0.1394\t0.146\t0.3488\t0.0954\n",
      "9. final_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv RFECV\twho\tclusters + AppCats + Words\tElasticNet\t0.1334\t0.1436\t0.3723\t0.1032\n",
      "10. final_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv RFECV\twho\tclusters + AppCats + RuLIWC + Words\tElasticNet\t0.1336\t0.1462\t0.332\t0.0796\n",
      "11. finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv RFECV\twho\tAppCats + RuLIWC + Behavior\tElasticNet\t0.1358\t0.145\t0.3363\t0.0835\n",
      "12. finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv RFECV\twho\tclusters + AppCats + RuLIWC + Behavior\tElasticNet\t0.1357\t0.1456\t0.3394\t0.0938\n",
      "13. finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv RFECV\twho\tAppCats + RuLIWC + Behavior + Words\tElasticNet\t0.1346\t0.1438\t0.367\t0.1193\n",
      "14. final_clust_reg4_testscores_max_Ridge_WHO_RFECV.tsv RFECV\twho\tclusters + AppCats + Words\tRidge\t0.1383\t0.1444\t0.338\t0.0894\n",
      "15. final_clust_reg4_testscores_max_Ridge_WHO_RFECV.tsv RFECV\twho\tclusters + RuLIWC + Words\tRidge\t0.1415\t0.1442\t0.3641\t0.1067\n",
      "AppCats 8\n",
      "RFECV who AppCats ElasticNet 0.1415 0.1509 0.2307 0.0308 final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who AppCats ElasticNet 0.1428 0.1511 0.2172 0.0329 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who AppCats Ridge 0.1457 0.1515 0.2408 0.0244 final_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who AppCats Lasso 0.1436 0.153 0.1836 0.0022 final_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who AppCats RandomForestRegressor 0.1446 0.1552 0.0977 -0.0202 final_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats LinearRegression 0.1505 0.1559 0.0498 -0.0301 final_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who AppCats AdaBoostRegressor 0.1405 0.1584 0.0386 -0.0905 final_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats DecisionTreeRegressor 0.1555 0.1606 -0.0711 -0.1265 final_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "\n",
      "RuLIWC 8\n",
      "RFECV who RuLIWC Lasso 0.1498 0.1529 0.1276 0.0197 final_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC ElasticNet 0.1498 0.1529 0.1276 0.0197 final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC LinearRegression 0.151 0.1534 0.1151 0.0108 final_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC Ridge 0.1496 0.1537 0.1471 0.0214 final_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC ElasticNet 0.1502 0.154 0.0877 0.0109 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC AdaBoostRegressor 0.1444 0.155 0.1571 -0.0039 final_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC RandomForestRegressor 0.1468 0.1563 0.1341 -0.0322 final_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC DecisionTreeRegressor 0.1579 0.1568 0.0385 -0.0866 final_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "\n",
      "Behavior 8\n",
      "RFECV who Behavior DecisionTreeRegressor 0.1518 0.1497 0.2463 0.0096 final_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who Behavior ElasticNet 0.1413 0.1509 0.1711 0.009 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who Behavior ElasticNet 0.1408 0.1517 0.1627 0.0104 final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who Behavior Lasso 0.1409 0.1518 0.1529 0.0082 final_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who Behavior Ridge 0.1415 0.152 0.1791 -0.0022 final_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who Behavior RandomForestRegressor 0.1419 0.1529 0.2205 0.0107 final_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who Behavior LinearRegression 0.1422 0.1554 0.1344 -0.0293 final_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who Behavior AdaBoostRegressor 0.1372 0.158 0.0636 -0.0679 final_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "\n",
      "Words 8\n",
      "RFECV who Words Lasso 0.1422 0.1441 0.3179 0.0817 final_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who Words ElasticNet 0.1382 0.1456 0.354 0.0864 final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who Words ElasticNet 0.1406 0.1466 0.2167 0.0734 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who Words Ridge 0.1415 0.1472 0.2847 0.0781 final_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who Words LinearRegression 0.1482 0.149 0.2295 0.0405 final_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who Words RandomForestRegressor 0.143 0.1502 0.186 0.0179 final_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who Words DecisionTreeRegressor 0.1537 0.1511 0.1519 -0.0348 final_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who Words AdaBoostRegressor 0.1278 0.1533 0.2014 -0.0094 final_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "\n",
      "AppCats + RuLIWC 8\n",
      "RFECV who AppCats + RuLIWC Ridge 0.1445 0.1505 0.2578 0.0371 final_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC RandomForestRegressor 0.1434 0.1511 0.1721 0.0087 final_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC ElasticNet 0.1405 0.1514 0.2623 0.0486 final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC ElasticNet 0.141 0.1518 0.2514 0.047 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC Lasso 0.1432 0.1527 0.2334 0.0055 final_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC DecisionTreeRegressor 0.159 0.1555 0.0233 -0.0614 final_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC AdaBoostRegressor 0.1349 0.1572 0.0821 -0.0666 final_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC LinearRegression 0.1496 130428324303.702 0.1098 -1.620766282744598e+26 final_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "\n",
      "AppCats + Behavior 8\n",
      "RFECV who AppCats + Behavior Lasso 0.1374 0.1458 0.2934 0.0678 final_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Behavior ElasticNet 0.1367 0.1495 0.3229 0.0547 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Behavior ElasticNet 0.1357 0.1497 0.2953 0.0251 final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Behavior Ridge 0.1399 0.15 0.233 0.0428 final_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Behavior DecisionTreeRegressor 0.1538 0.1517 0.2163 -0.0097 final_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Behavior LinearRegression 0.1461 0.153 0.1206 0.0014 final_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Behavior AdaBoostRegressor 0.1346 0.1537 0.1439 -0.0253 final_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Behavior RandomForestRegressor 0.141 0.1571 0.1778 -0.03 final_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "\n",
      "AppCats + Words 8\n",
      "RFECV who AppCats + Words ElasticNet 0.1345 0.1456 0.3581 0.0995 final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Words ElasticNet 0.1378 0.1458 0.3228 0.0772 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Words AdaBoostRegressor 0.1288 0.1479 0.2469 0.034 final_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Words RandomForestRegressor 0.1429 0.1483 0.2533 0.048 final_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Words LinearRegression 0.1497 0.1483 0.2526 0.0442 final_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Words Lasso 0.1389 0.1503 0.2535 0.0474 final_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Words DecisionTreeRegressor 0.154 0.152 0.1365 -0.0481 final_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Words Ridge 0.1385 0.1558 0.2113 -0.1973 final_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "\n",
      "RuLIWC + Behavior 8\n",
      "RFECV who RuLIWC + Behavior DecisionTreeRegressor 0.1513 0.1505 0.2399 0.0032 final_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Behavior Lasso 0.141 0.1507 0.1878 0.0271 final_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Behavior ElasticNet 0.1403 0.151 0.1881 0.0228 final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Behavior ElasticNet 0.1414 0.1519 0.1511 -0.0002 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Behavior Ridge 0.1411 0.1521 0.2166 0.0077 final_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Behavior LinearRegression 0.1437 0.1535 0.1712 -0.0158 final_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Behavior RandomForestRegressor 0.1434 0.1539 0.1698 -0.0112 final_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Behavior AdaBoostRegressor 0.135 0.155 0.1252 -0.0455 final_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "\n",
      "RuLIWC + Words 8\n",
      "RFECV who RuLIWC + Words ElasticNet 0.1394 0.1438 0.3729 0.1058 final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Words Ridge 0.1421 0.1445 0.3242 0.0964 final_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Words ElasticNet 0.1409 0.147 0.2091 0.071 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Words Lasso 0.1435 0.1472 0.2729 0.0689 final_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Words LinearRegression 0.1506 0.1473 0.2684 0.0677 final_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Words RandomForestRegressor 0.1432 0.1489 0.2298 0.0488 final_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Words DecisionTreeRegressor 0.1537 0.1511 0.1519 -0.0348 final_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Words AdaBoostRegressor 0.1295 0.1516 0.2622 0.0301 final_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "\n",
      "Behavior + Words 8\n",
      "RFECV who Behavior + Words ElasticNet 0.1352 0.1456 0.294 0.0753 final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who Behavior + Words AdaBoostRegressor 0.1252 0.1473 0.2813 0.0476 final_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who Behavior + Words LinearRegression 0.1458 0.1474 0.2336 0.0595 final_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who Behavior + Words Lasso 0.1364 0.1478 0.2753 0.055 final_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who Behavior + Words Ridge 0.1365 0.1479 0.2065 0.0453 final_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who Behavior + Words ElasticNet 0.1381 0.1496 0.1922 0.0142 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who Behavior + Words DecisionTreeRegressor 0.157 0.1505 0.2147 -0.0034 final_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who Behavior + Words RandomForestRegressor 0.1432 0.1539 0.1924 0.0047 final_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "\n",
      "AppCats + RuLIWC + Behavior 8\n",
      "RFECV who AppCats + RuLIWC + Behavior ElasticNet 0.1358 0.145 0.3363 0.0835 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Behavior Lasso 0.137 0.1455 0.3271 0.0849 final_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Behavior ElasticNet 0.1352 0.1475 0.3151 0.0507 final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Behavior Ridge 0.1391 0.1487 0.2919 0.0517 final_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Behavior DecisionTreeRegressor 0.1538 0.1517 0.2163 -0.0097 final_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Behavior RandomForestRegressor 0.1408 0.1586 0.1502 -0.0596 final_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Behavior AdaBoostRegressor 0.1332 0.1594 0.1163 -0.0822 final_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Behavior LinearRegression 0.1459 173072444692.9913 0.1663 -2.853858397556792e+26 final_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "\n",
      "AppCats + RuLIWC + Words 8\n",
      "RFECV who AppCats + RuLIWC + Words Ridge 0.14 0.146 0.3222 0.0817 final_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Words ElasticNet 0.1357 0.1471 0.2997 0.065 final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Words RandomForestRegressor 0.1435 0.1478 0.2293 0.0329 final_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Words AdaBoostRegressor 0.1262 0.1484 0.2773 0.0531 final_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Words ElasticNet 0.1368 0.1484 0.2487 0.0482 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Words Lasso 0.1386 0.1493 0.2871 0.0448 final_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Words LinearRegression 0.1493 0.1507 0.1747 0.0089 final_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Words DecisionTreeRegressor 0.154 0.152 0.1365 -0.0481 final_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "\n",
      "AppCats + Behavior + Words 8\n",
      "RFECV who AppCats + Behavior + Words ElasticNet 0.1341 0.1452 0.3152 0.0975 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Behavior + Words Lasso 0.1369 0.1464 0.3636 0.074 final_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Behavior + Words ElasticNet 0.1326 0.1467 0.3316 0.0731 final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Behavior + Words Ridge 0.135 0.148 0.2547 0.0604 final_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Behavior + Words LinearRegression 0.149 0.1487 0.2497 0.046 final_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Behavior + Words RandomForestRegressor 0.1434 0.1503 0.2005 0.0163 final_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Behavior + Words DecisionTreeRegressor 0.1579 0.152 0.2021 -0.0184 final_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats + Behavior + Words AdaBoostRegressor 0.1257 0.1542 0.2177 -0.0003 final_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "\n",
      "RuLIWC + Behavior + Words 8\n",
      "RFECV who RuLIWC + Behavior + Words ElasticNet 0.1367 0.1479 0.2531 0.0531 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Behavior + Words ElasticNet 0.1357 0.1482 0.2709 0.0572 final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Behavior + Words Ridge 0.1363 0.1493 0.2264 0.0283 final_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Behavior + Words DecisionTreeRegressor 0.157 0.1505 0.2147 -0.0034 final_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Behavior + Words Lasso 0.1377 0.1507 0.2245 0.0411 final_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Behavior + Words LinearRegression 0.1484 0.1508 0.2176 0.0127 final_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Behavior + Words AdaBoostRegressor 0.1246 0.1515 0.2405 -0.0093 final_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who RuLIWC + Behavior + Words RandomForestRegressor 0.1434 0.1532 0.1609 -0.007 final_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "\n",
      "AppCats + RuLIWC + Behavior + Words 8\n",
      "RFECV who AppCats + RuLIWC + Behavior + Words ElasticNet 0.1346 0.1438 0.367 0.1193 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Behavior + Words ElasticNet 0.1325 0.1461 0.3592 0.0999 final_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Behavior + Words Ridge 0.1365 0.1496 0.262 0.0415 final_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Behavior + Words Lasso 0.1362 0.1504 0.3229 0.0331 final_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Behavior + Words RandomForestRegressor 0.1439 0.1508 0.1929 0.0153 final_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Behavior + Words DecisionTreeRegressor 0.1579 0.152 0.2021 -0.0184 final_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Behavior + Words AdaBoostRegressor 0.1263 0.1565 0.1641 -0.037 final_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who AppCats + RuLIWC + Behavior + Words LinearRegression 0.1522 0.1665 0.1494 -1.2836 final_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "\n",
      "clusters 8\n",
      "RFECV who clusters Lasso 0.1497 0.1516 0.1533 0.0241 final_clust_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who clusters ElasticNet 0.1493 0.1519 0.1465 0.023 final_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters ElasticNet 0.1496 0.152 0.14 0.0217 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters Ridge 0.1496 0.1523 0.1907 0.0196 final_clust_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who clusters AdaBoostRegressor 0.14 0.1524 0.1597 -0.0074 final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters LinearRegression 0.151 0.1534 0.0779 -0.0093 final_clust_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who clusters DecisionTreeRegressor 0.1583 0.1538 0.0876 -0.0424 final_clust_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters RandomForestRegressor 0.1459 0.1566 0.0586 -0.0569 final_clust_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "\n",
      "clusters + AppCats 8\n",
      "RFECV who clusters + AppCats ElasticNet 0.141 0.1502 0.2537 0.0492 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats Ridge 0.1448 0.152 0.1971 0.015 final_clust_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats LinearRegression 0.1487 0.1526 0.0997 0.0008 final_clust_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats Lasso 0.1416 0.1531 0.1956 -0.0101 final_clust_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats DecisionTreeRegressor 0.157 0.1534 0.0944 -0.0365 final_clust_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats ElasticNet 0.1396 0.1537 0.2314 0.0048 final_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats RandomForestRegressor 0.1464 0.1564 0.0685 -0.0433 final_clust_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats AdaBoostRegressor 0.1369 0.1628 -0.0022 -0.1352 final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "\n",
      "clusters + RuLIWC 8\n",
      "RFECV who clusters + RuLIWC AdaBoostRegressor 0.1384 0.1527 0.1822 -0.007 final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC DecisionTreeRegressor 0.1569 0.1538 0.0876 -0.0424 final_clust_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC ElasticNet 0.148 0.1555 0.1941 -0.0088 final_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC Ridge 0.148 0.1555 0.1943 -0.0086 final_clust_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC Lasso 0.1484 0.1556 0.1871 -0.006 final_clust_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC ElasticNet 0.1483 0.1556 0.1615 -0.0117 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC LinearRegression 0.1501 0.1566 0.1223 -0.0593 final_clust_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC RandomForestRegressor 0.1449 0.1572 0.0768 -0.055 final_clust_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "\n",
      "clusters + Behavior 8\n",
      "RFECV who clusters + Behavior DecisionTreeRegressor 0.154 0.15 0.2343 -0.0026 final_clust_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + Behavior ElasticNet 0.1425 0.1522 0.1824 0.0163 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + Behavior ElasticNet 0.1409 0.1525 0.1391 -0.0 final_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + Behavior Lasso 0.1412 0.1526 0.1338 -0.0026 final_clust_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who clusters + Behavior RandomForestRegressor 0.1438 0.1527 0.1722 -0.0063 final_clust_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + Behavior AdaBoostRegressor 0.1365 0.1544 0.1886 -0.0065 final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + Behavior Ridge 0.141 0.1549 0.1413 -0.0292 final_clust_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who clusters + Behavior LinearRegression 0.1437 0.1593 0.0955 -0.093 final_clust_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "\n",
      "clusters + Words 8\n",
      "RFECV who clusters + Words ElasticNet 0.1429 0.1449 0.2628 0.0975 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + Words Ridge 0.1422 0.1455 0.3266 0.0877 final_clust_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who clusters + Words ElasticNet 0.1394 0.146 0.3488 0.0954 final_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + Words Lasso 0.1415 0.1468 0.2964 0.0908 final_clust_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who clusters + Words LinearRegression 0.1444 0.1468 0.2875 0.0791 final_clust_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who clusters + Words AdaBoostRegressor 0.1288 0.1492 0.2431 0.027 final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + Words RandomForestRegressor 0.1431 0.1508 0.2041 0.0294 final_clust_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + Words DecisionTreeRegressor 0.1537 0.1511 0.1519 -0.0348 final_clust_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "\n",
      "clusters + AppCats + RuLIWC 8\n",
      "RFECV who clusters + AppCats + RuLIWC ElasticNet 0.1411 0.1493 0.2807 0.0786 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC ElasticNet 0.1405 0.1501 0.2885 0.0684 final_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC DecisionTreeRegressor 0.1559 0.1535 0.1134 -0.0574 final_clust_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC Lasso 0.1425 0.1537 0.1955 -0.0064 final_clust_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC Ridge 0.1446 0.1549 0.2183 -0.0036 final_clust_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC AdaBoostRegressor 0.1359 0.1556 0.1735 -0.0324 final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC LinearRegression 0.1479 0.1559 0.0994 -0.0355 final_clust_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC RandomForestRegressor 0.1446 0.1573 0.0695 -0.0413 final_clust_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "\n",
      "clusters + AppCats + Behavior 8\n",
      "RFECV who clusters + AppCats + Behavior ElasticNet 0.1361 0.1469 0.3013 0.0739 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Behavior ElasticNet 0.1354 0.148 0.2867 0.0584 final_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Behavior Lasso 0.1373 0.1498 0.2525 0.0324 final_clust_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Behavior LinearRegression 0.1472 0.1507 0.1747 0.0315 final_clust_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Behavior Ridge 0.1401 0.1508 0.2367 0.0434 final_clust_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Behavior DecisionTreeRegressor 0.1538 0.1517 0.2163 -0.0097 final_clust_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Behavior AdaBoostRegressor 0.1355 0.1541 0.174 -0.0211 final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Behavior RandomForestRegressor 0.1419 0.1553 0.1798 -0.0275 final_clust_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "\n",
      "clusters + AppCats + Words 8\n",
      "RFECV who clusters + AppCats + Words ElasticNet 0.1334 0.1436 0.3723 0.1032 final_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Words Ridge 0.1383 0.1444 0.338 0.0894 final_clust_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Words Lasso 0.1382 0.1464 0.2994 0.0825 final_clust_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Words LinearRegression 0.15 0.1475 0.2385 0.0662 final_clust_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Words ElasticNet 0.1368 0.1482 0.2886 0.0498 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Words AdaBoostRegressor 0.1277 0.1495 0.2514 0.0246 final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Words DecisionTreeRegressor 0.154 0.152 0.1365 -0.0481 final_clust_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Words RandomForestRegressor 0.1435 0.1537 0.1322 -0.014 final_clust_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "\n",
      "clusters + RuLIWC + Behavior 8\n",
      "RFECV who clusters + RuLIWC + Behavior DecisionTreeRegressor 0.1513 0.1505 0.2399 0.0032 final_clust_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Behavior RandomForestRegressor 0.1441 0.1529 0.1725 0.0014 final_clust_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Behavior Lasso 0.141 0.1546 0.163 -0.0112 final_clust_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Behavior ElasticNet 0.1407 0.1546 0.1556 -0.0249 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Behavior AdaBoostRegressor 0.1355 0.1549 0.1629 -0.0426 final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Behavior ElasticNet 0.14 0.155 0.1711 -0.0225 final_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Behavior Ridge 0.1409 0.1553 0.215 -0.0161 final_clust_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Behavior LinearRegression 0.1437 0.1614 0.1383 -0.0976 final_clust_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "\n",
      "clusters + RuLIWC + Words 8\n",
      "RFECV who clusters + RuLIWC + Words AdaBoostRegressor 0.1289 0.1436 0.3202 0.081 final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Words Ridge 0.1415 0.1442 0.3641 0.1067 final_clust_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Words Lasso 0.1443 0.1455 0.3164 0.0879 final_clust_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Words ElasticNet 0.1414 0.1457 0.2279 0.0774 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Words ElasticNet 0.1391 0.1472 0.3125 0.0774 final_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Words LinearRegression 0.1489 0.1486 0.2911 0.0241 final_clust_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Words RandomForestRegressor 0.1433 0.1506 0.22 0.0382 final_clust_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Words DecisionTreeRegressor 0.1537 0.1511 0.1519 -0.0348 final_clust_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "\n",
      "clusters + Behavior + Words 8\n",
      "RFECV who clusters + Behavior + Words Ridge 0.1364 0.1462 0.2389 0.0653 final_clust_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who clusters + Behavior + Words ElasticNet 0.1359 0.1471 0.2594 0.0503 final_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + Behavior + Words LinearRegression 0.1467 0.1472 0.2483 0.0711 final_clust_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who clusters + Behavior + Words Lasso 0.1383 0.1475 0.2276 0.0443 final_clust_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who clusters + Behavior + Words ElasticNet 0.1389 0.1491 0.2148 0.0366 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + Behavior + Words DecisionTreeRegressor 0.157 0.1505 0.2147 -0.0034 final_clust_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + Behavior + Words RandomForestRegressor 0.1431 0.1528 0.1798 -0.0025 final_clust_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + Behavior + Words AdaBoostRegressor 0.1266 0.156 0.194 -0.0348 final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "\n",
      "clusters + AppCats + RuLIWC + Behavior 8\n",
      "RFECV who clusters + AppCats + RuLIWC + Behavior ElasticNet 0.1357 0.1456 0.3394 0.0938 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Behavior ElasticNet 0.1354 0.1486 0.2996 0.0592 final_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Behavior Ridge 0.1393 0.1504 0.2834 0.0237 final_clust_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Behavior Lasso 0.1372 0.1508 0.2753 0.0383 final_clust_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Behavior DecisionTreeRegressor 0.1538 0.1517 0.2163 -0.0097 final_clust_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Behavior LinearRegression 0.1462 0.156 0.1175 -0.0255 final_clust_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Behavior RandomForestRegressor 0.1425 0.1586 0.1325 -0.0572 final_clust_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Behavior AdaBoostRegressor 0.1353 0.1592 0.0856 -0.0983 final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "\n",
      "clusters + AppCats + RuLIWC + Words 8\n",
      "RFECV who clusters + AppCats + RuLIWC + Words ElasticNet 0.1336 0.1462 0.332 0.0796 final_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Words ElasticNet 0.135 0.1472 0.3088 0.0716 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Words LinearRegression 0.1448 0.148 0.2804 0.0496 final_clust_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Words Lasso 0.1382 0.1484 0.2538 0.0153 final_clust_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Words Ridge 0.139 0.1484 0.2738 0.0531 final_clust_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Words DecisionTreeRegressor 0.154 0.152 0.1365 -0.0481 final_clust_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Words RandomForestRegressor 0.1438 0.1533 0.1555 -0.0037 final_clust_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Words AdaBoostRegressor 0.1276 0.1596 0.1331 -0.0813 final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "\n",
      "clusters + AppCats + Behavior + Words 8\n",
      "RFECV who clusters + AppCats + Behavior + Words Lasso 0.1373 0.1457 0.3339 0.0701 final_clust_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Behavior + Words Ridge 0.1367 0.1469 0.2601 0.0659 final_clust_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Behavior + Words ElasticNet 0.134 0.1482 0.287 0.0515 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Behavior + Words LinearRegression 0.1505 0.1483 0.2331 0.0529 final_clust_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Behavior + Words ElasticNet 0.1323 0.1495 0.2708 0.023 final_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Behavior + Words AdaBoostRegressor 0.1259 0.1499 0.2676 0.0374 final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Behavior + Words RandomForestRegressor 0.1435 0.1509 0.2067 0.0204 final_clust_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + Behavior + Words DecisionTreeRegressor 0.1579 0.152 0.2021 -0.0184 final_clust_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "\n",
      "clusters + RuLIWC + Behavior + Words 8\n",
      "RFECV who clusters + RuLIWC + Behavior + Words ElasticNet 0.1386 0.1478 0.2961 0.072 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Behavior + Words AdaBoostRegressor 0.127 0.1481 0.2619 0.0339 final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Behavior + Words LinearRegression 0.1462 0.1493 0.2314 0.0464 final_clust_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Behavior + Words ElasticNet 0.1366 0.1498 0.2578 0.0458 final_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Behavior + Words Lasso 0.1387 0.1504 0.2576 0.0452 final_clust_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Behavior + Words Ridge 0.1372 0.1504 0.201 0.0221 final_clust_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Behavior + Words DecisionTreeRegressor 0.157 0.1505 0.2147 -0.0034 final_clust_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + RuLIWC + Behavior + Words RandomForestRegressor 0.1435 0.1519 0.1694 -0.0021 final_clust_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "\n",
      "clusters + AppCats + RuLIWC + Behavior + Words 7\n",
      "RFECV who clusters + AppCats + RuLIWC + Behavior + Words ElasticNet 0.1328 0.148 0.2952 0.0544 finalNEW_clust_reg4_testscores_max_ElasticNet_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Behavior + Words Lasso 0.1348 0.1481 0.3131 0.0535 final_clust_reg4_testscores_max_Lasso_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Behavior + Words Ridge 0.1366 0.1481 0.2645 0.053 final_clust_reg4_testscores_max_Ridge_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Behavior + Words LinearRegression 0.1467 0.1489 0.1914 0.0512 final_clust_reg4_testscores_max_LinearRegression_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Behavior + Words RandomForestRegressor 0.1434 0.1508 0.2234 0.0356 final_clust_reg4_testscores_max_RandomForestRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Behavior + Words DecisionTreeRegressor 0.1579 0.152 0.2021 -0.0184 final_clust_reg4_testscores_max_DecisionTreeRegressor_WHO_RFECV.tsv\n",
      "RFECV who clusters + AppCats + RuLIWC + Behavior + Words AdaBoostRegressor 0.1265 0.1521 0.2607 0.0175 final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vieweval('WHO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. final_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv RFECV\tdiener\tAppCats + Behavior + Words\tElasticNet\t0.1628\t0.1746\t0.3688\t0.0848\n",
      "2. final_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv RFECV\tdiener\tRuLIWC + Behavior + Words\tElasticNet\t0.1667\t0.1767\t0.3387\t0.0769\n",
      "3. finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv RFECV\tdiener\tWords\tElasticNet\t0.1706\t0.1744\t0.3402\t0.1022\n",
      "4. finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv RFECV\tdiener\tclusters + Words\tElasticNet\t0.1714\t0.1716\t0.3627\t0.0901\n",
      "5. finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv RFECV\tdiener\tRuLIWC + Words\tElasticNet\t0.1699\t0.1722\t0.352\t0.0988\n",
      "6. finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv RFECV\tdiener\tclusters + RuLIWC + Words\tElasticNet\t0.1712\t0.1712\t0.3673\t0.1192\n",
      "7. finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv RFECV\tdiener\tclusters + Behavior + Words\tElasticNet\t0.1695\t0.1712\t0.3459\t0.1228\n",
      "8. finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv RFECV\tdiener\tRuLIWC + Behavior + Words\tElasticNet\t0.1696\t0.1752\t0.3506\t0.0934\n",
      "9. finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv RFECV\tdiener\tclusters + AppCats + Behavior + Words\tElasticNet\t0.1648\t0.1698\t0.4024\t0.1045\n",
      "10. finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv RFECV\tdiener\tclusters + AppCats + RuLIWC + Behavior + Words\tElasticNet\t0.1654\t0.1681\t0.3776\t0.1164\n",
      "11. final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv RFECV\tdiener\tclusters + Words\tElasticNet\t0.167\t0.171\t0.3594\t0.1129\n",
      "12. final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv RFECV\tdiener\tclusters + RuLIWC + Words\tElasticNet\t0.1684\t0.1711\t0.3446\t0.1075\n",
      "13. final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv RFECV\tdiener\tclusters + Behavior + Words\tElasticNet\t0.1663\t0.1691\t0.3718\t0.1164\n",
      "14. final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv RFECV\tdiener\tclusters + AppCats + Behavior + Words\tElasticNet\t0.1617\t0.1704\t0.4002\t0.1136\n",
      "15. final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv RFECV\tdiener\tclusters + Words\tLasso\t0.1702\t0.1715\t0.3435\t0.112\n",
      "16. final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv RFECV\tdiener\tclusters + RuLIWC + Words\tLasso\t0.1713\t0.1714\t0.3354\t0.1109\n",
      "17. final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv RFECV\tdiener\tclusters + AppCats + RuLIWC + Behavior + Words\tLasso\t0.1631\t0.1741\t0.338\t0.0796\n",
      "18. final_clust_reg4_testscores_max_Ridge_Diener_RFECV.tsv RFECV\tdiener\tclusters + AppCats + RuLIWC + Behavior + Words\tRidge\t0.1666\t0.1774\t0.3422\t0.078\n",
      "AppCats 8\n",
      "RFECV diener AppCats ElasticNet 0.1706 0.1762 0.2737 0.0172 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener AppCats ElasticNet 0.1692 0.179 0.2434 0.0066 final_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener AppCats Lasso 0.1736 0.1815 0.1307 0.0123 final_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener AppCats Ridge 0.1747 0.183 0.1624 -0.0065 final_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener AppCats AdaBoostRegressor 0.1698 0.1833 0.1468 0.0047 final_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "RFECV diener AppCats LinearRegression 0.182 0.187 0.009 -0.0206 final_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener AppCats RandomForestRegressor 0.1775 0.1885 0.0316 -0.0515 final_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener AppCats DecisionTreeRegressor 0.1938 0.1924 -0.0382 -0.1045 final_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "\n",
      "RuLIWC 8\n",
      "RFECV diener RuLIWC DecisionTreeRegressor 0.1844 0.182 0.2168 0.0142 final_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC AdaBoostRegressor 0.1727 0.1834 0.1948 0.0151 final_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC Lasso 0.1798 0.1835 0.1538 0.0198 final_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC LinearRegression 0.1808 0.184 0.1664 0.0102 final_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC ElasticNet 0.1795 0.1841 0.1534 0.013 final_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC Ridge 0.1794 0.1842 0.1509 0.0124 final_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC ElasticNet 0.1801 0.1843 0.1554 0.0123 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC RandomForestRegressor 0.1742 0.1847 0.1745 0.0061 final_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "\n",
      "Behavior 8\n",
      "RFECV diener Behavior DecisionTreeRegressor 0.1801 0.1785 0.191 0.0195 final_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener Behavior AdaBoostRegressor 0.1694 0.185 0.1419 -0.011 final_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "RFECV diener Behavior Lasso 0.1737 0.1856 0.1216 -0.018 final_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener Behavior ElasticNet 0.1729 0.1856 0.1075 -0.0206 final_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener Behavior ElasticNet 0.1731 0.1857 0.1009 -0.0235 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener Behavior RandomForestRegressor 0.1734 0.1862 0.068 -0.0298 final_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener Behavior Ridge 0.1737 0.1865 0.0774 -0.0401 final_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener Behavior LinearRegression 0.1751 0.1919 0.023 -0.1192 final_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "\n",
      "Words 8\n",
      "RFECV diener Words ElasticNet 0.1706 0.1744 0.3402 0.1022 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener Words Ridge 0.1704 0.1752 0.3159 0.0801 final_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener Words ElasticNet 0.1677 0.1764 0.3142 0.067 final_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener Words Lasso 0.1731 0.177 0.2576 0.0586 final_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener Words LinearRegression 0.18 0.1793 0.2427 0.045 final_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener Words DecisionTreeRegressor 0.1812 0.1815 0.1227 -0.0159 final_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener Words RandomForestRegressor 0.1733 0.1841 0.1591 -0.0007 final_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener Words AdaBoostRegressor 0.1586 0.1856 0.1907 -0.0042 final_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "AppCats + RuLIWC 8\n",
      "RFECV diener AppCats + RuLIWC ElasticNet 0.1698 0.1776 0.2478 0.0296 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC Ridge 0.1734 0.1793 0.23 0.0463 final_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC ElasticNet 0.1679 0.1811 0.211 -0.0133 final_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC DecisionTreeRegressor 0.1966 0.1816 0.1653 0.0059 final_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC Lasso 0.1712 0.1818 0.1809 0.0117 final_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC LinearRegression 0.1777 0.1841 0.1262 -0.0507 final_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC RandomForestRegressor 0.1769 0.1846 0.1084 -0.019 final_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC AdaBoostRegressor 0.1667 0.1896 0.0982 -0.0679 final_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "AppCats + Behavior 8\n",
      "RFECV diener AppCats + Behavior ElasticNet 0.1681 0.1784 0.2227 0.0248 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Behavior ElasticNet 0.1649 0.1786 0.2763 0.0391 final_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Behavior Lasso 0.1667 0.1791 0.2768 0.0355 final_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Behavior Ridge 0.1693 0.1806 0.2473 0.0235 final_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Behavior DecisionTreeRegressor 0.1913 0.1818 0.1837 0.0018 final_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Behavior RandomForestRegressor 0.1742 0.1848 0.1727 -0.0049 final_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Behavior AdaBoostRegressor 0.1598 0.1884 0.1156 -0.0535 final_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Behavior LinearRegression 0.1755 0.191 0.1955 -0.1129 final_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "\n",
      "AppCats + Words 8\n",
      "RFECV diener AppCats + Words Ridge 0.1678 0.1756 0.2992 0.0864 final_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Words LinearRegression 0.1777 0.1776 0.2813 0.0761 final_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Words Lasso 0.1706 0.178 0.232 0.0281 final_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Words ElasticNet 0.1657 0.1786 0.2615 0.0084 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Words ElasticNet 0.1646 0.1796 0.266 0.0106 final_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Words DecisionTreeRegressor 0.1854 0.1817 0.1191 -0.0251 final_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Words RandomForestRegressor 0.1724 0.1851 0.1453 -0.0011 final_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Words AdaBoostRegressor 0.1616 0.1934 0.0358 -0.1239 final_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "RuLIWC + Behavior 8\n",
      "RFECV diener RuLIWC + Behavior DecisionTreeRegressor 0.1841 0.1818 0.1949 0.0133 final_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Behavior ElasticNet 0.1732 0.1849 0.1451 -0.0229 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Behavior Ridge 0.1731 0.1854 0.1307 -0.0327 final_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Behavior Lasso 0.1733 0.1857 0.1271 -0.0279 final_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Behavior ElasticNet 0.1727 0.186 0.1132 -0.0336 final_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Behavior AdaBoostRegressor 0.1693 0.1879 0.0955 -0.0591 final_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Behavior RandomForestRegressor 0.1733 0.1899 0.096 -0.0528 final_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Behavior LinearRegression 0.1744 0.191 0.0245 -0.0871 final_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "\n",
      "RuLIWC + Words 8\n",
      "RFECV diener RuLIWC + Words ElasticNet 0.1699 0.1722 0.352 0.0988 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Words ElasticNet 0.1687 0.1742 0.3068 0.07 final_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Words Lasso 0.1729 0.1748 0.2556 0.0785 final_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Words Ridge 0.1704 0.176 0.3054 0.0813 final_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Words LinearRegression 0.1762 0.1778 0.2998 0.0639 final_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Words DecisionTreeRegressor 0.184 0.18 0.1591 0.0071 final_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Words RandomForestRegressor 0.1694 0.1843 0.1095 -0.0105 final_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Words AdaBoostRegressor 0.16 0.1873 0.1424 -0.0521 final_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "Behavior + Words 8\n",
      "RFECV diener Behavior + Words ElasticNet 0.1695 0.1754 0.314 0.0752 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener Behavior + Words Ridge 0.1687 0.1791 0.2582 0.042 final_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener Behavior + Words RandomForestRegressor 0.1719 0.18 0.2044 0.0356 final_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener Behavior + Words DecisionTreeRegressor 0.182 0.1809 0.1367 -0.005 final_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener Behavior + Words LinearRegression 0.1809 0.1814 0.1826 -0.0164 final_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener Behavior + Words ElasticNet 0.1673 0.1828 0.293 -0.0181 final_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener Behavior + Words Lasso 0.1705 0.1847 0.2257 -0.0305 final_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener Behavior + Words AdaBoostRegressor 0.1603 0.1947 0.0356 -0.1365 final_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "AppCats + RuLIWC + Behavior 8\n",
      "RFECV diener AppCats + RuLIWC + Behavior ElasticNet 0.1662 0.1761 0.3093 0.0704 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Behavior Ridge 0.1677 0.1775 0.2784 0.0775 final_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Behavior ElasticNet 0.1643 0.1786 0.2618 0.0401 final_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Behavior Lasso 0.1663 0.1798 0.267 0.0163 final_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Behavior DecisionTreeRegressor 0.196 0.1823 0.177 -0.0032 final_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Behavior LinearRegression 0.1757 0.1827 0.1716 0.0334 final_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Behavior RandomForestRegressor 0.1756 0.1867 0.1183 -0.0175 final_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Behavior AdaBoostRegressor 0.164 0.1873 0.1232 -0.0549 final_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "AppCats + RuLIWC + Words 8\n",
      "RFECV diener AppCats + RuLIWC + Words Lasso 0.1685 0.1753 0.2913 0.0711 final_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Words Ridge 0.1683 0.1757 0.2758 0.0764 final_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Words ElasticNet 0.1668 0.1768 0.2738 0.0384 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Words ElasticNet 0.1634 0.1798 0.272 0.0025 final_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Words LinearRegression 0.1818 0.1805 0.2174 0.028 final_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Words RandomForestRegressor 0.1728 0.1824 0.1718 0.0125 final_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Words DecisionTreeRegressor 0.1885 0.1836 0.1229 -0.0346 final_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Words AdaBoostRegressor 0.1587 0.1921 0.1181 -0.0885 final_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "AppCats + Behavior + Words 8\n",
      "RFECV diener AppCats + Behavior + Words ElasticNet 0.1659 0.1735 0.3004 0.0724 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Behavior + Words ElasticNet 0.1628 0.1746 0.3688 0.0848 final_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Behavior + Words Ridge 0.1663 0.1767 0.2911 0.0755 final_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Behavior + Words Lasso 0.1662 0.1793 0.2991 0.0257 final_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Behavior + Words LinearRegression 0.1812 0.1797 0.2461 0.0188 final_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Behavior + Words DecisionTreeRegressor 0.186 0.181 0.1179 -0.0257 final_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Behavior + Words RandomForestRegressor 0.1722 0.1861 0.1235 -0.0098 final_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + Behavior + Words AdaBoostRegressor 0.1597 0.1896 0.0502 -0.1137 final_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "RuLIWC + Behavior + Words 8\n",
      "RFECV diener RuLIWC + Behavior + Words ElasticNet 0.1696 0.1752 0.3506 0.0934 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Behavior + Words ElasticNet 0.1667 0.1767 0.3387 0.0769 final_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Behavior + Words RandomForestRegressor 0.1698 0.1797 0.2099 0.0396 final_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Behavior + Words Lasso 0.1704 0.1806 0.2089 0.0352 final_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Behavior + Words LinearRegression 0.1818 0.1808 0.2516 0.0302 final_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Behavior + Words DecisionTreeRegressor 0.1849 0.182 0.1207 -0.0153 final_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Behavior + Words Ridge 0.1681 0.182 0.2411 0.017 final_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener RuLIWC + Behavior + Words AdaBoostRegressor 0.1594 0.1889 0.0713 -0.1022 final_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "AppCats + RuLIWC + Behavior + Words 7\n",
      "RFECV diener AppCats + RuLIWC + Behavior + Words ElasticNet 0.1648 0.1719 0.3255 0.0968 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Behavior + Words Lasso 0.1637 0.1764 0.3219 0.0713 final_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Behavior + Words Ridge 0.1662 0.1777 0.2871 0.0758 final_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Behavior + Words DecisionTreeRegressor 0.1891 0.183 0.1053 -0.0421 final_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Behavior + Words LinearRegression 0.1827 0.1836 0.1443 0.0085 final_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Behavior + Words AdaBoostRegressor 0.1586 0.1855 0.1892 -0.0042 final_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "RFECV diener AppCats + RuLIWC + Behavior + Words RandomForestRegressor 0.1719 0.1857 0.1291 -0.0044 final_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "\n",
      "clusters 8\n",
      "RFECV diener clusters RandomForestRegressor 0.1733 0.1814 0.1709 0.026 final_clust_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters DecisionTreeRegressor 0.1838 0.1838 0.1266 -0.0254 final_clust_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters Ridge 0.1817 0.1859 0.0321 -0.007 final_clust_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener clusters ElasticNet 0.1814 0.1865 0.015 -0.0214 final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters Lasso 0.182 0.1865 0.041 -0.0144 final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener clusters ElasticNet 0.1821 0.1869 -0.009 -0.0154 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters AdaBoostRegressor 0.1711 0.1889 0.0317 -0.0705 final_clust_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters LinearRegression 0.1837 0.19 -0.0574 -0.0669 final_clust_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "\n",
      "clusters + AppCats 8\n",
      "RFECV diener clusters + AppCats ElasticNet 0.1703 0.1786 0.2545 0.0129 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats Ridge 0.1747 0.1825 0.1663 -0.006 final_clust_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats RandomForestRegressor 0.1729 0.1833 0.1828 0.01 final_clust_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats ElasticNet 0.1684 0.1835 0.2003 -0.0386 final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats Lasso 0.1712 0.1836 0.1064 -0.0672 final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats LinearRegression 0.1824 0.1866 -0.0273 -0.0437 final_clust_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats AdaBoostRegressor 0.1638 0.1911 0.0124 -0.1063 final_clust_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats DecisionTreeRegressor 0.1938 0.1924 -0.0382 -0.1045 final_clust_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "\n",
      "clusters + RuLIWC 8\n",
      "RFECV diener clusters + RuLIWC DecisionTreeRegressor 0.1796 0.1769 0.2769 0.0507 final_clust_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC ElasticNet 0.1811 0.1839 0.1121 0.0014 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC RandomForestRegressor 0.1651 0.1847 0.1659 -0.0017 final_clust_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC Ridge 0.1783 0.186 0.1068 -0.0152 final_clust_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC AdaBoostRegressor 0.1649 0.1861 0.1751 -0.0088 final_clust_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC Lasso 0.1787 0.1864 0.15 -0.0096 final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC ElasticNet 0.1786 0.1868 0.1232 -0.0146 final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC LinearRegression 0.1838 0.1899 -0.0291 -0.053 final_clust_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "\n",
      "clusters + Behavior 8\n",
      "RFECV diener clusters + Behavior DecisionTreeRegressor 0.1822 0.1765 0.2243 0.0368 final_clust_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Behavior RandomForestRegressor 0.1688 0.1837 0.1562 -0.0071 final_clust_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Behavior Ridge 0.172 0.1855 0.1174 -0.022 final_clust_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Behavior Lasso 0.1738 0.1858 0.1025 -0.0199 final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Behavior ElasticNet 0.1719 0.186 0.1021 -0.0299 final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Behavior ElasticNet 0.1725 0.1861 0.0921 -0.0331 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Behavior LinearRegression 0.1797 0.1926 -0.0432 -0.0922 final_clust_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Behavior AdaBoostRegressor 0.1667 0.1948 0.0222 -0.0907 final_clust_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "clusters + Words 8\n",
      "RFECV diener clusters + Words ElasticNet 0.167 0.171 0.3594 0.1129 final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Words Lasso 0.1702 0.1715 0.3435 0.112 final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Words ElasticNet 0.1714 0.1716 0.3627 0.0901 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Words Ridge 0.1701 0.1731 0.325 0.1066 final_clust_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Words DecisionTreeRegressor 0.18 0.1825 0.1097 -0.0234 final_clust_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Words RandomForestRegressor 0.1711 0.1868 0.0972 -0.0274 final_clust_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Words LinearRegression 0.1777 0.1873 0.2135 -0.2455 final_clust_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Words AdaBoostRegressor 0.1579 0.1892 0.1007 -0.0618 final_clust_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "clusters + AppCats + RuLIWC 8\n",
      "RFECV diener clusters + AppCats + RuLIWC ElasticNet 0.17 0.1778 0.2636 0.0314 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC DecisionTreeRegressor 0.1978 0.1816 0.1653 0.0059 final_clust_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC RandomForestRegressor 0.1741 0.1817 0.1685 0.0091 final_clust_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC ElasticNet 0.1676 0.1819 0.194 -0.0079 final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC Ridge 0.1731 0.1819 0.1558 0.0173 final_clust_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC Lasso 0.1708 0.1872 0.0756 -0.0412 final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC LinearRegression 0.1813 0.1878 -0.0382 -0.0264 final_clust_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC AdaBoostRegressor 0.1624 0.189 0.079 -0.092 final_clust_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "clusters + AppCats + Behavior 8\n",
      "RFECV diener clusters + AppCats + Behavior ElasticNet 0.1659 0.1756 0.2341 0.0528 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Behavior Ridge 0.17 0.1775 0.2585 0.053 final_clust_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Behavior ElasticNet 0.1634 0.1778 0.2316 0.0519 final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Behavior DecisionTreeRegressor 0.1918 0.1812 0.184 0.002 final_clust_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Behavior Lasso 0.1651 0.1835 0.1926 -0.0628 final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Behavior LinearRegression 0.1809 0.1875 -0.0447 -0.0382 final_clust_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Behavior RandomForestRegressor 0.1702 0.1893 0.0862 -0.0707 final_clust_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Behavior AdaBoostRegressor 0.1616 0.192 0.0042 -0.1259 final_clust_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "clusters + AppCats + Words 8\n",
      "RFECV diener clusters + AppCats + Words Lasso 0.1695 0.1712 0.2958 0.0932 final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Words ElasticNet 0.1669 0.1751 0.3111 0.0684 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Words Ridge 0.1678 0.1751 0.3064 0.0901 final_clust_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Words ElasticNet 0.1644 0.1753 0.32 0.0658 final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Words DecisionTreeRegressor 0.1843 0.1826 0.1061 -0.0327 final_clust_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Words LinearRegression 0.1801 0.1835 0.1996 -0.0888 final_clust_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Words RandomForestRegressor 0.1715 0.1854 0.164 0.0056 final_clust_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Words AdaBoostRegressor 0.1597 0.1897 0.1267 -0.0694 final_clust_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "clusters + RuLIWC + Behavior 8\n",
      "RFECV diener clusters + RuLIWC + Behavior DecisionTreeRegressor 0.1823 0.1765 0.2275 0.038 final_clust_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Behavior Lasso 0.1728 0.1849 0.1225 -0.0141 final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Behavior ElasticNet 0.1712 0.1851 0.1085 -0.0165 final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Behavior ElasticNet 0.1725 0.187 0.0929 -0.0538 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Behavior RandomForestRegressor 0.1692 0.1876 0.1058 -0.0318 final_clust_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Behavior Ridge 0.171 0.1891 0.0918 -0.0642 final_clust_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Behavior AdaBoostRegressor 0.1648 0.1925 0.0873 -0.0787 final_clust_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Behavior LinearRegression 0.1789 0.1981 -0.0864 -0.1649 final_clust_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "\n",
      "clusters + RuLIWC + Words 8\n",
      "RFECV diener clusters + RuLIWC + Words ElasticNet 0.1684 0.1711 0.3446 0.1075 final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Words ElasticNet 0.1712 0.1712 0.3673 0.1192 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Words Lasso 0.1713 0.1714 0.3354 0.1109 final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Words Ridge 0.1703 0.1749 0.309 0.0896 final_clust_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Words LinearRegression 0.1802 0.1763 0.2973 0.0854 final_clust_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Words RandomForestRegressor 0.1675 0.1796 0.2576 0.0611 final_clust_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Words DecisionTreeRegressor 0.184 0.18 0.1591 0.0071 final_clust_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Words AdaBoostRegressor 0.1597 0.1822 0.1568 -0.0295 final_clust_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "clusters + Behavior + Words 8\n",
      "RFECV diener clusters + Behavior + Words ElasticNet 0.1663 0.1691 0.3718 0.1164 final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Behavior + Words ElasticNet 0.1695 0.1712 0.3459 0.1228 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Behavior + Words Lasso 0.1683 0.1736 0.3297 0.0993 final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Behavior + Words Ridge 0.1679 0.1772 0.2763 0.0404 final_clust_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Behavior + Words LinearRegression 0.1796 0.1783 0.2287 0.0326 final_clust_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Behavior + Words DecisionTreeRegressor 0.1808 0.1819 0.1237 -0.0126 final_clust_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Behavior + Words AdaBoostRegressor 0.1612 0.1867 0.1398 -0.0506 final_clust_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + Behavior + Words RandomForestRegressor 0.1717 0.1869 0.1238 -0.01 final_clust_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "\n",
      "clusters + AppCats + RuLIWC + Behavior 8\n",
      "RFECV diener clusters + AppCats + RuLIWC + Behavior ElasticNet 0.1665 0.1748 0.2962 0.0048 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Behavior ElasticNet 0.1635 0.1785 0.24 -0.0261 final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Behavior DecisionTreeRegressor 0.196 0.1823 0.177 -0.0032 final_clust_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Behavior Ridge 0.1687 0.1831 0.2314 -0.0181 final_clust_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Behavior RandomForestRegressor 0.1744 0.1839 0.1683 0.0094 final_clust_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Behavior Lasso 0.1664 0.1844 0.2179 -0.0221 final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Behavior LinearRegression 0.1795 0.1928 -0.0176 -0.1251 final_clust_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Behavior AdaBoostRegressor 0.1624 0.1942 0.0513 -0.1258 final_clust_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "clusters + AppCats + RuLIWC + Words 8\n",
      "RFECV diener clusters + AppCats + RuLIWC + Words Ridge 0.1679 0.1751 0.2882 0.0811 final_clust_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Words ElasticNet 0.1667 0.1754 0.2947 0.061 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Words ElasticNet 0.163 0.1767 0.3208 0.0474 final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Words Lasso 0.1673 0.1786 0.2685 0.0419 final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Words LinearRegression 0.1822 0.1793 0.1822 0.04 final_clust_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Words RandomForestRegressor 0.1735 0.1803 0.2541 0.0557 final_clust_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Words DecisionTreeRegressor 0.1885 0.1836 0.1229 -0.0346 final_clust_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Words AdaBoostRegressor 0.1585 0.1879 0.1252 -0.0625 final_clust_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "clusters + AppCats + Behavior + Words 8\n",
      "RFECV diener clusters + AppCats + Behavior + Words ElasticNet 0.1648 0.1698 0.4024 0.1045 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Behavior + Words ElasticNet 0.1617 0.1704 0.4002 0.1136 final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Behavior + Words Lasso 0.1639 0.1749 0.3047 0.061 final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Behavior + Words Ridge 0.1666 0.1764 0.2855 0.0673 final_clust_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Behavior + Words RandomForestRegressor 0.1727 0.1805 0.195 0.023 final_clust_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Behavior + Words DecisionTreeRegressor 0.1852 0.1813 0.1119 -0.0255 final_clust_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Behavior + Words LinearRegression 0.1775 0.1871 0.1864 -0.1225 final_clust_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + Behavior + Words AdaBoostRegressor 0.1632 0.1918 0.0649 -0.1124 final_clust_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "clusters + RuLIWC + Behavior + Words 8\n",
      "RFECV diener clusters + RuLIWC + Behavior + Words ElasticNet 0.1657 0.1765 0.3136 0.0631 final_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Behavior + Words Lasso 0.1678 0.1776 0.294 0.0616 final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Behavior + Words ElasticNet 0.1693 0.1787 0.2724 0.0486 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Behavior + Words RandomForestRegressor 0.1699 0.1796 0.231 0.0448 final_clust_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Behavior + Words LinearRegression 0.1769 0.1809 0.2016 0.0148 final_clust_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Behavior + Words Ridge 0.1686 0.1814 0.2426 -0.0044 final_clust_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Behavior + Words DecisionTreeRegressor 0.1849 0.182 0.1207 -0.0153 final_clust_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + RuLIWC + Behavior + Words AdaBoostRegressor 0.1597 0.1947 0.0783 -0.1117 final_clust_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n",
      "clusters + AppCats + RuLIWC + Behavior + Words 7\n",
      "RFECV diener clusters + AppCats + RuLIWC + Behavior + Words ElasticNet 0.1654 0.1681 0.3776 0.1164 finalNEW_clust_reg4_testscores_max_ElasticNet_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Behavior + Words Lasso 0.1631 0.1741 0.338 0.0796 final_clust_reg4_testscores_max_Lasso_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Behavior + Words Ridge 0.1666 0.1774 0.3422 0.078 final_clust_reg4_testscores_max_Ridge_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Behavior + Words LinearRegression 0.1783 0.1787 0.2496 0.0618 final_clust_reg4_testscores_max_LinearRegression_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Behavior + Words RandomForestRegressor 0.1733 0.179 0.2098 0.0394 final_clust_reg4_testscores_max_RandomForestRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Behavior + Words DecisionTreeRegressor 0.1891 0.183 0.1053 -0.0421 final_clust_reg4_testscores_max_DecisionTreeRegressor_Diener_RFECV.tsv\n",
      "RFECV diener clusters + AppCats + RuLIWC + Behavior + Words AdaBoostRegressor 0.1571 0.1872 0.1112 -0.0626 final_clust_reg4_testscores_max_AdaBoostRegressor_Diener_RFECV.tsv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vieweval('Diener')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv RFECV\twho\tWords\tAdaBoostRegressor\t0.1402\t0.1472\t0.3021\t0.054\n",
      "2. final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv RFECV\twho\tRuLIWC + Behavior + Words\tAdaBoostRegressor\t0.1425\t0.1461\t0.3148\t0.0679\n",
      "clusters 7\n",
      "RFECV who clusters RandomForestRegressor 0.1511 0.1553 0.065 -0.0339\n",
      "AppCats 7\n",
      "RFECV who AppCats RandomForestRegressor 0.1489 0.1534 0.0675 -0.0223\n",
      "RuLIWC 7\n",
      "RFECV who RuLIWC ElasticNet 0.1507 0.1524 0.1831 0.0279\n",
      "Behavior 7\n",
      "RFECV who Behavior Ridge 0.1477 0.1525 0.1522 -0.0042\n",
      "Words 7\n",
      "RFECV who Words AdaBoostRegressor 0.1402 0.1472 0.3021 0.054\n",
      "clusters + AppCats 7\n",
      "RFECV who clusters + AppCats RandomForestRegressor 0.1496 0.1561 0.0336 -0.0617\n",
      "clusters + RuLIWC 7\n",
      "RFECV who clusters + RuLIWC Lasso 0.1491 0.1546 0.0652 -0.0048\n",
      "clusters + Behavior 7\n",
      "RFECV who clusters + Behavior RandomForestRegressor 0.1506 0.1522 0.1365 -0.0028\n",
      "clusters + Words 7\n",
      "RFECV who clusters + Words ElasticNet 0.148 0.1516 0.1991 0.0042\n",
      "AppCats + RuLIWC 7\n",
      "RFECV who AppCats + RuLIWC Ridge 0.1541 0.1549 -0.0877 -0.0101\n",
      "AppCats + Behavior 7\n",
      "RFECV who AppCats + Behavior ElasticNet 0.1499 0.1518 0.1743 0.0146\n",
      "AppCats + Words 7\n",
      "RFECV who AppCats + Words AdaBoostRegressor 0.1426 0.1511 0.2342 0.0041\n",
      "RuLIWC + Behavior 7\n",
      "RFECV who RuLIWC + Behavior AdaBoostRegressor 0.146 0.1489 0.2001 0.0089\n",
      "RuLIWC + Words 7\n",
      "RFECV who RuLIWC + Words Ridge 0.1484 0.1506 0.212 0.0362\n",
      "Behavior + Words 7\n",
      "RFECV who Behavior + Words AdaBoostRegressor 0.1407 0.1519 0.2746 0.0271\n",
      "clusters + AppCats + RuLIWC 7\n",
      "RFECV who clusters + AppCats + RuLIWC RandomForestRegressor 0.1516 0.1567 0.0323 -0.0491\n",
      "clusters + AppCats + Behavior 7\n",
      "RFECV who clusters + AppCats + Behavior ElasticNet 0.1476 0.1529 0.1864 0.0152\n",
      "clusters + AppCats + Words 7\n",
      "RFECV who clusters + AppCats + Words AdaBoostRegressor 0.1447 0.1496 0.2243 0.0168\n",
      "clusters + RuLIWC + Behavior 7\n",
      "RFECV who clusters + RuLIWC + Behavior RandomForestRegressor 0.1512 0.1542 0.1189 -0.0226\n",
      "clusters + RuLIWC + Words 7\n",
      "RFECV who clusters + RuLIWC + Words RandomForestRegressor 0.1502 0.1512 0.1657 0.0113\n",
      "clusters + Behavior + Words 7\n",
      "RFECV who clusters + Behavior + Words AdaBoostRegressor 0.1449 0.1512 0.2211 -0.0051\n",
      "AppCats + RuLIWC + Behavior 7\n",
      "RFECV who AppCats + RuLIWC + Behavior AdaBoostRegressor 0.1494 0.1523 0.1666 -0.0158\n",
      "AppCats + RuLIWC + Words 7\n",
      "RFECV who AppCats + RuLIWC + Words AdaBoostRegressor 0.1441 0.1499 0.2498 0.0359\n",
      "AppCats + Behavior + Words 7\n",
      "RFECV who AppCats + Behavior + Words AdaBoostRegressor 0.1438 0.1504 0.2466 0.0314\n",
      "RuLIWC + Behavior + Words 7\n",
      "RFECV who RuLIWC + Behavior + Words AdaBoostRegressor 0.1425 0.1461 0.3148 0.0679\n",
      "clusters + AppCats + RuLIWC + Behavior 7\n",
      "RFECV who clusters + AppCats + RuLIWC + Behavior ElasticNet 0.1476 0.1528 0.1909 0.0091\n",
      "clusters + AppCats + RuLIWC + Words 7\n",
      "RFECV who clusters + AppCats + RuLIWC + Words AdaBoostRegressor 0.1449 0.152 0.2122 0.0123\n",
      "clusters + AppCats + Behavior + Words 7\n",
      "RFECV who clusters + AppCats + Behavior + Words AdaBoostRegressor 0.1404 0.1507 0.2148 0.008\n",
      "clusters + RuLIWC + Behavior + Words 7\n",
      "RFECV who clusters + RuLIWC + Behavior + Words AdaBoostRegressor 0.1441 0.1495 0.2463 0.0379\n",
      "AppCats + RuLIWC + Behavior + Words 7\n",
      "RFECV who AppCats + RuLIWC + Behavior + Words AdaBoostRegressor 0.1433 0.1502 0.2762 0.0439\n",
      "clusters + AppCats + RuLIWC + Behavior + Words 7\n",
      "RFECV who clusters + AppCats + RuLIWC + Behavior + Words Lasso 0.1526 0.1526 0.0796 0.0006\n"
     ]
    }
   ],
   "source": [
    "vieweval('WHO', './res_april2021_noRFE/', thresh=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusters 7\n",
      "RFECV diener clusters Ridge 0.1803 0.1862 0.0108 -0.017\n",
      "AppCats 7\n",
      "RFECV diener AppCats Lasso 0.1839 0.1874 0.0074 -0.0207\n",
      "RuLIWC 7\n",
      "RFECV diener RuLIWC Lasso 0.1809 0.1824 0.1392 0.0243\n",
      "Behavior 7\n",
      "RFECV diener Behavior DecisionTreeRegressor 0.1806 0.1806 0.144 -0.0098\n",
      "Words 7\n",
      "RFECV diener Words Ridge 0.1785 0.1829 0.1285 0.0202\n",
      "clusters + AppCats 7\n",
      "RFECV diener clusters + AppCats Lasso 0.1848 0.1859 -0.0065 -0.0098\n",
      "clusters + RuLIWC 7\n",
      "RFECV diener clusters + RuLIWC ElasticNet 0.1794 0.1832 0.1246 0.0012\n",
      "clusters + Behavior 7\n",
      "RFECV diener clusters + Behavior DecisionTreeRegressor 0.1847 0.1847 0.1354 -0.0262\n",
      "clusters + Words 7\n",
      "RFECV diener clusters + Words Ridge 0.1787 0.1846 0.0578 -0.0051\n",
      "AppCats + RuLIWC 7\n",
      "RFECV diener AppCats + RuLIWC AdaBoostRegressor 0.1808 0.1888 0.0601 -0.064\n",
      "AppCats + Behavior 7\n",
      "RFECV diener AppCats + Behavior Ridge 0.1804 0.1866 0.0741 -0.0337\n",
      "AppCats + Words 7\n",
      "RFECV diener AppCats + Words Ridge 0.1803 0.1833 0.1562 0.0169\n",
      "RuLIWC + Behavior 7\n",
      "RFECV diener RuLIWC + Behavior DecisionTreeRegressor 0.1892 0.1892 0.0866 -0.0922\n",
      "RuLIWC + Words 7\n",
      "RFECV diener RuLIWC + Words Ridge 0.1775 0.1855 0.0949 -0.0107\n",
      "Behavior + Words 7\n",
      "RFECV diener Behavior + Words ElasticNet 0.1772 0.1862 0.1455 -0.036\n",
      "clusters + AppCats + RuLIWC 7\n",
      "RFECV diener clusters + AppCats + RuLIWC Lasso 0.1848 0.1859 -0.0065 -0.0098\n",
      "clusters + AppCats + Behavior 7\n",
      "RFECV diener clusters + AppCats + Behavior Lasso 0.1853 0.1853 0.0 -0.0013\n",
      "clusters + AppCats + Words 7\n",
      "RFECV diener clusters + AppCats + Words Ridge 0.1809 0.1853 0.1585 0.0037\n",
      "clusters + RuLIWC + Behavior 7\n",
      "RFECV diener clusters + RuLIWC + Behavior Ridge 0.1796 0.1888 0.1121 -0.0427\n",
      "clusters + RuLIWC + Words 7\n",
      "RFECV diener clusters + RuLIWC + Words Ridge 0.1782 0.1841 0.0827 0.0006\n",
      "clusters + Behavior + Words 7\n",
      "RFECV diener clusters + Behavior + Words Ridge 0.1782 0.1862 0.1261 -0.0225\n",
      "AppCats + RuLIWC + Behavior 7\n",
      "RFECV diener AppCats + RuLIWC + Behavior AdaBoostRegressor 0.1802 0.1857 0.0868 -0.0485\n",
      "AppCats + RuLIWC + Words 7\n",
      "RFECV diener AppCats + RuLIWC + Words Ridge 0.1801 0.1832 0.1682 0.0197\n",
      "AppCats + Behavior + Words 7\n",
      "RFECV diener AppCats + Behavior + Words Ridge 0.1797 0.1833 0.1614 0.0025\n",
      "RuLIWC + Behavior + Words 7\n",
      "RFECV diener RuLIWC + Behavior + Words AdaBoostRegressor 0.176 0.1836 0.1685 -0.0413\n",
      "clusters + AppCats + RuLIWC + Behavior 7\n",
      "RFECV diener clusters + AppCats + RuLIWC + Behavior Lasso 0.1853 0.1853 0.0 -0.0013\n",
      "clusters + AppCats + RuLIWC + Words 7\n",
      "RFECV diener clusters + AppCats + RuLIWC + Words Ridge 0.1808 0.1852 0.1677 0.0055\n",
      "clusters + AppCats + Behavior + Words 7\n",
      "RFECV diener clusters + AppCats + Behavior + Words Ridge 0.1804 0.1844 0.1707 0.0104\n",
      "clusters + RuLIWC + Behavior + Words 7\n",
      "RFECV diener clusters + RuLIWC + Behavior + Words RandomForestRegressor 0.1801 0.1838 0.1545 0.0108\n",
      "AppCats + RuLIWC + Behavior + Words 7\n",
      "RFECV diener AppCats + RuLIWC + Behavior + Words Ridge 0.1795 0.1831 0.1717 0.0062\n",
      "clusters + AppCats + RuLIWC + Behavior + Words 7\n",
      "RFECV diener clusters + AppCats + RuLIWC + Behavior + Words Ridge 0.1803 0.183 0.1731 0.0166\n"
     ]
    }
   ],
   "source": [
    "vieweval('Diener', './res_april2021_noRFE/', thresh=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv RFECV\twho\tWords\tAdaBoostRegressor\t0.1402\t0.1472\t0.3021\t0.054\n",
      "2. final_clust_reg4_testscores_max_AdaBoostRegressor_WHO_RFECV.tsv RFECV\twho\tRuLIWC + Behavior + Words\tAdaBoostRegressor\t0.1425\t0.1461\t0.3148\t0.0679\n"
     ]
    }
   ],
   "source": [
    "rfoo = './res_april2021_noRFE/'\n",
    "i = 0\n",
    "for fn in os.listdir(rfoo):\n",
    "    if 'testscore' in fn:\n",
    "        #print(fn)\n",
    "        f = open(rfoo+fn, 'r')\n",
    "        for line in f:\n",
    "            if line.startswith('method'):\n",
    "                continue\n",
    "            l = line.strip().split('\\t')\n",
    "            #print(l[-1])\n",
    "            if float(l[-2]) > 0.3:\n",
    "                i+=1\n",
    "                print(str(i)+'.', fn, line.strip())\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
